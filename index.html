<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="Loy With No Title">
<meta property="og:url" content="https://loyf.github.io/index.html">
<meta property="og:site_name" content="Loy With No Title">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Loy With No Title">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://loyf.github.io/">





  <title>Loy With No Title</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Loy With No Title</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Life can not be planned</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://loyf.github.io/2019/05/30/Python-Spark-《同桌的你》歌曲评论听众信息分析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Loy Fan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Loy With No Title">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/30/Python-Spark-《同桌的你》歌曲评论听众信息分析/" itemprop="url">Python + Spark 《同桌的你》歌曲评论听众信息分析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-30T15:39:02+08:00">
                2019-05-30
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Big-Data/" itemprop="url" rel="index">
                    <span itemprop="name">Big Data</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<blockquote>
<p>Fan Shiqing @Xiamen University</p>
</blockquote>
<h1 id="实验环境安装"><a href="#实验环境安装" class="headerlink" title="实验环境安装"></a>实验环境安装</h1><p>Linux：Ubuntu16.04<br>Java：1.7.0_80<br>Hadoop：2.7.1<br>Python：2.7<br>PyCharm：2019.1.2(Community Edition)<br>matplotlib：2.0.0<br>Spark：2.1.0<br><img src="https://img-blog.csdnimg.cn/20190528144036571.png" width="55%" alt></p>
<h1 id="下载数据集"><a href="#下载数据集" class="headerlink" title="下载数据集"></a>下载数据集</h1><p>数据集为某音乐平台歌曲《同桌的你》评论者的信息数据，包含评论者的用户ID、动态总数、关注总数、粉丝总数、所在地区、个人介绍、年龄、累计听歌总数属性。共4752条数据，部分如下图：<br><img src="https://img-blog.csdnimg.cn/20190528143330174.png" width="90%" alt></p>
<h1 id="数据集的预处理"><a href="#数据集的预处理" class="headerlink" title="数据集的预处理"></a>数据集的预处理</h1><ul>
<li>将txt文件转为csv文件</li>
<li>修改文件属性名称方便读写<br><img src="https://img-blog.csdnimg.cn/20190528144906725.png" width="90%" alt></li>
</ul>
<h1 id="使用Spark进行数据分析"><a href="#使用Spark进行数据分析" class="headerlink" title="使用Spark进行数据分析"></a>使用Spark进行数据分析</h1><ul>
<li><p>读入数据并筛选需要用到的属性</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sc =SparkContext()</span><br><span class="line">sqlContext = SQLContext(sc)</span><br><span class="line">data = sqlContext.read.format(<span class="string">'com.databricks.spark.csv'</span>).options(header=<span class="string">'true'</span>, inferschema=<span class="string">'true'</span>).load(<span class="string">'commenters.csv'</span>)</span><br><span class="line">list = [<span class="string">'ID'</span>, <span class="string">'fans'</span>, <span class="string">'province'</span>, <span class="string">'city'</span>, <span class="string">'age'</span>, <span class="string">'songs'</span>]</span><br><span class="line">data = data.select([column <span class="keyword">for</span> column <span class="keyword">in</span> data.columns <span class="keyword">if</span> column <span class="keyword">in</span> list])</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看数据、显示数据的结构</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">"【展示10行数据】"</span></span><br><span class="line">data.show(<span class="number">10</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"【数据结构】"</span></span><br><span class="line">data.printSchema()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">【展示10行数据】</span><br><span class="line">19/05/28 14:18:41 WARN util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting &apos;spark.debug.maxToStringFields&apos; in SparkEnv.conf.</span><br><span class="line">+---------+----+--------+----+----+-----+</span><br><span class="line">|       ID|fans|province|city| age|songs|</span><br><span class="line">+---------+----+--------+----+----+-----+</span><br><span class="line">|132708526|   0|     浙江省| 金华市|  19| 1142|</span><br><span class="line">|126403842|   7|      海外|  其它|   0| 1033|</span><br><span class="line">|358013382|   3|     山东省| 烟台市|未知年龄|  232|</span><br><span class="line">| 31322471|   8|     河南省| 郑州市|未知年龄|  341|</span><br><span class="line">|398405071|   0|     安徽省| 合肥市|未知年龄|  141|</span><br><span class="line">|321142743|   0|     山东省| 临沂市|未知年龄|   75|</span><br><span class="line">|252135807|  13|     山东省| 烟台市|未知年龄|  704|</span><br><span class="line">| 10729784|  11|     浙江省| 杭州市|   1| 5585|</span><br><span class="line">|116980492|   8|     重庆市| 万州区|未知年龄|  174|</span><br><span class="line">|118405606|   0|     陕西省| 汉中市|  27| 1343|</span><br><span class="line">+---------+----+--------+----+----+-----+</span><br><span class="line">only showing top 10 rows</span><br><span class="line"></span><br><span class="line">【数据结构】</span><br><span class="line">root</span><br><span class="line"> |-- ID: integer (nullable = true)</span><br><span class="line"> |-- fans: integer (nullable = true)</span><br><span class="line"> |-- province: string (nullable = true)</span><br><span class="line"> |-- city: string (nullable = true)</span><br><span class="line"> |-- age: string (nullable = true)</span><br><span class="line"> |-- songs: string (nullable = true)</span><br></pre></td></tr></table></figure>
<ul>
<li>将年龄、听歌数量改为integer类型，以便进行数据统计<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = data.withColumn(<span class="string">"age"</span>, data[<span class="string">"age"</span>].cast(IntegerType()))</span><br><span class="line">data = data.withColumn(<span class="string">"songs"</span>, data[<span class="string">"songs"</span>].cast(IntegerType()))``</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">【数据类型转换】</span><br><span class="line">root</span><br><span class="line"> |-- ID: integer (nullable = true)</span><br><span class="line"> |-- fans: integer (nullable = true)</span><br><span class="line"> |-- province: string (nullable = true)</span><br><span class="line"> |-- city: string (nullable = true)</span><br><span class="line"> |-- age: integer (nullable = true)</span><br><span class="line"> |-- songs: integer (nullable = true)</span><br></pre></td></tr></table></figure>
<ul>
<li>查找评论者当中听歌最多的人的歌曲数量<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.select(max(<span class="string">'songs'</span>)).show()</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">【评论者最多听歌数量】</span><br><span class="line">+----------+</span><br><span class="line">|max(songs)|</span><br><span class="line">+----------+</span><br><span class="line">|     35180|</span><br><span class="line">+----------+</span><br></pre></td></tr></table></figure>
<ul>
<li>统计《同桌的你》的评论者所在地区的分布情况</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">area = data.groupBy(<span class="string">'province'</span>).count().orderBy(col(<span class="string">"count"</span>).desc())</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">【评论者所在地区分布】</span><br><span class="line">+--------+-----+</span><br><span class="line">|province|count|</span><br><span class="line">+--------+-----+</span><br><span class="line">|     广东省|  534|</span><br><span class="line">|      海外|  271|</span><br><span class="line">|     四川省|  263|</span><br><span class="line">|     山东省|  263|</span><br><span class="line">|     江苏省|  254|</span><br><span class="line">|     河南省|  245|</span><br><span class="line">|     北京市|  217|</span><br><span class="line">|     浙江省|  207|</span><br><span class="line">|      新疆|  205|</span><br><span class="line">|     安徽省|  193|</span><br><span class="line">|     湖南省|  188|</span><br><span class="line">|     湖北省|  185|</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">|     青海省|   15|</span><br><span class="line">|     台湾省|    9|</span><br><span class="line">|      西藏|    5|</span><br><span class="line">|      澳门|    5|</span><br><span class="line">+--------+-----+</span><br></pre></td></tr></table></figure>
<ul>
<li>统计《同桌的你》的评论者的年龄的分布情况<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">age = data.groupBy(<span class="string">'age'</span>).count().orderBy(col(<span class="string">"count"</span>).desc())</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">【评论者年龄TOP10】</span><br><span class="line">+----+-----+</span><br><span class="line">| age|count|</span><br><span class="line">+----+-----+</span><br><span class="line">|null| 2451|</span><br><span class="line">|  21|  245|</span><br><span class="line">|  27|  228|</span><br><span class="line">|  19|  228|</span><br><span class="line">|  20|  215|</span><br><span class="line">|  22|  204|</span><br><span class="line">|  23|  163|</span><br><span class="line">|  18|  154|</span><br><span class="line">|  17|  153|</span><br><span class="line">|  24|  115|</span><br><span class="line">+----+-----+</span><br></pre></td></tr></table></figure>
<ul>
<li>统计《同桌的你》的评论者所在各个地区的听众平均年龄<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mean = data.groupBy(<span class="string">'province'</span>).agg(&#123;<span class="string">"age"</span>: <span class="string">"mean"</span>&#125;)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">【各个地区平均年龄情况】</span><br><span class="line">+--------+------------------+</span><br><span class="line">|province|          avg(age)|</span><br><span class="line">+--------+------------------+</span><br><span class="line">|     北京市|22.432692307692307|</span><br><span class="line">|      海外|19.030075187969924|</span><br><span class="line">|     辽宁省| 22.69811320754717|</span><br><span class="line">|     浙江省| 21.06451612903226|</span><br><span class="line">|     内蒙古|             20.75|</span><br><span class="line">|      新疆| 20.00943396226415|</span><br><span class="line">|     海南省|20.272727272727273|</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">|     吉林省|            22.375|</span><br><span class="line">|    未知地区|3850.4285714285716|</span><br><span class="line">|     上海市| 22.50793650793651|</span><br><span class="line">|      澳门|              27.0|</span><br><span class="line">|     青海省|20.333333333333332|</span><br><span class="line">|     江西省|19.904109589041095|</span><br><span class="line">|     安徽省| 20.22826086956522|</span><br><span class="line">|     江苏省|21.293233082706767|</span><br><span class="line">|     云南省|22.295454545454547|</span><br><span class="line">+--------+------------------+</span><br></pre></td></tr></table></figure>
<ul>
<li>统计《同桌的你》的评论者的粉丝数量情况<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fans = data.groupBy(<span class="string">'fans'</span>).count().orderBy(col(<span class="string">"count"</span>).desc())</span><br></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">【评论者粉丝数量】</span><br><span class="line">+----+-----+</span><br><span class="line">|fans|count|</span><br><span class="line">+----+-----+</span><br><span class="line">|   0| 1163|</span><br><span class="line">|   1|  714|</span><br><span class="line">|   2|  494|</span><br><span class="line">|   3|  351|</span><br><span class="line">|   4|  283|</span><br><span class="line">|   5|  205|</span><br><span class="line">|   6|  173|</span><br><span class="line">|   7|  149|</span><br><span class="line">|   8|  110|</span><br><span class="line">|   9|  101|</span><br><span class="line">|  10|   84|</span><br><span class="line">+----+-----+</span><br></pre></td></tr></table></figure>
<h1 id="可视化呈现"><a href="#可视化呈现" class="headerlink" title="可视化呈现"></a>可视化呈现</h1><p>将数据分析的结果通过matplotlib可视化显示出来<br>（在实验时，中文一直出错无法显示，暂用拼音和英文代替）</p>
<ul>
<li><p><strong>《同桌的你》的听众所在地区的分布情况</strong><br>从图中看出，这首歌曲广东省的听众远超过其他地区。<br><img src="https://img-blog.csdnimg.cn/2019052815520791.png" width="55%" alt></p>
</li>
<li><p><strong>《同桌的你》听众年龄分布</strong><br>从图中看出，这首歌的听众20-30岁之间的居多，另外33岁的听众非常多。<br>有部分因素是网络歌曲平台的受众大部分在于这个年龄阶段。<br><img src="https://img-blog.csdnimg.cn/20190528155154880.png" width="55%" alt></p>
</li>
</ul>
<ul>
<li><p><strong>《同桌的你》各个区域听众平均年龄</strong><br>平均年龄基本上在17-27岁之间。<br><img src="https://img-blog.csdnimg.cn/20190528155212436.png" width="55%" alt></p>
</li>
<li><p><strong>听众粉丝数量情况</strong><br>大部分听众的粉丝数量低于10。<br><img src="https://img-blog.csdnimg.cn/20190528155219480.png" width="55%" alt></p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://loyf.github.io/2019/05/30/Python-numpy实现同态加密算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Loy Fan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Loy With No Title">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/30/Python-numpy实现同态加密算法/" itemprop="url">Python + numpy实现同态加密算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-30T15:37:17+08:00">
                2019-05-30
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Cryptography/" itemprop="url" rel="index">
                    <span itemprop="name">Cryptography</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<p>在实现之前，先了解一下什么是<a href="https://blog.csdn.net/weixin_43318626/article/details/89977202" target="_blank" rel="noopener">同态加密</a>。</p>
<p>以下这个公式使我们要实现的同态加密版本</p>
<script type="math/tex; mode=display">
S \mathbf{c}=w \mathbf{x}+\mathbf{e} \quad \mathbf{x}=\left\lceil\frac{S \mathbf{c}}{w}\right\rfloor</script><p><strong>第一种加密</strong></p>
<p>对称加密</p>
<p>如果密钥S是一个单位矩阵，那么c不过是输入x的一个重加权的、略带噪声的版本。</p>
<p>当S为单位矩阵时，相当于没有加密</p>
<p>当S为随机矩阵时，有加密</p>
<p>加解密使用同一key</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_key</span><span class="params">(w,m,n)</span>:</span></span><br><span class="line">    S = (np.random.rand(m,n) * w / (<span class="number">2</span> ** <span class="number">16</span>))<span class="comment"># 可证明 max(S) &lt; w</span></span><br><span class="line">    <span class="keyword">return</span> S<span class="comment"># key，对称加密</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encrypt</span><span class="params">(x,S,m,n,w)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(x) == len(S)</span><br><span class="line">    e = (np.random.rand(m))<span class="comment"># 可证明 max(e) &lt; w / 2</span></span><br><span class="line">    c = np.linalg.inv(S).dot((w * x) + e)   </span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decrypt</span><span class="params">(c,S,w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (S.dot(c) / w).astype(<span class="string">'int'</span>)</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>])</span><br><span class="line">m = len(x)</span><br><span class="line">n = m</span><br><span class="line">w = <span class="number">16</span></span><br><span class="line">S = generate_key(w,m,n)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">c = encrypt(x,S,m,n,w)</span><br><span class="line">decrypt(c,S,w)</span><br></pre></td></tr></table></figure>
<p>array([0, 1, 2, 5])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(x+x)</span><br><span class="line">print(x*<span class="number">10</span>)</span><br></pre></td></tr></table></figure>
<p>[ 0  2  4 10]<br>[ 0 10 20 50]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">decrypt(c+c,S,w)</span><br></pre></td></tr></table></figure>
<p>array([ 0,  2,  4, 10])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">decrypt(c*<span class="number">10</span>,S,w)</span><br></pre></td></tr></table></figure>
<p>array([ 0, 10, 20, 50])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x*x</span><br></pre></td></tr></table></figure>
<p>array([0, 1, 4])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">decrypt(c*c,S,w)</span><br></pre></td></tr></table></figure>
<p>array([15296055,  6267577,  8584289])</p>
<p><strong>第二种加密</strong></p>
<p>论文作者没有显式地分配一对独立的“公钥”和“私钥”，相反，提出了一种“钥交换”技术，将私钥S替换为S’。更具体地，这一私钥交换技术涉及生成一个可以进行该变换的矩阵M。由于M具备将消息从未加密状态（单位矩阵密钥）转换为加密状态（随机而难以猜测的密钥），这个M矩阵正好可以用作我们的公钥！</p>
<p>基于开篇两个公式，如果密钥是一个单位矩阵，那么消息是未加密的。</p>
<p>基于开篇两个公式，如果密钥是一个随机矩阵，那么消息是加密的。</p>
<p>我们构造一个矩阵M将一个密钥转换为另一个私钥。</p>
<p>当矩阵M将单位矩阵转换为一个随机密钥时，根据定义，它使用单向加密方式加密了消息。</p>
<p>由于M充当了“单向加密”的角色，我们称它为“公钥”，并且可以像公钥一样分发它，因为它无法用于解密。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_key</span><span class="params">(w,m,n)</span>:</span></span><br><span class="line">    S = (np.random.rand(m,n) * w / (<span class="number">2</span> ** <span class="number">16</span>)) <span class="comment"># 可证明 max(S) &lt; w</span></span><br><span class="line">    <span class="keyword">return</span> S</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encrypt</span><span class="params">(x,S,m,n,w)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> len(x) == len(S)</span><br><span class="line">    e = (np.random.rand(m)) <span class="comment"># 可证明 max(e) &lt; w / 2</span></span><br><span class="line">    c = np.linalg.inv(S).dot((w * x) + e)</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decrypt</span><span class="params">(c,S,w)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (S.dot(c) / w).astype(<span class="string">'int'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_c_star</span><span class="params">(c,m,l)</span>:</span></span><br><span class="line">    c_star = np.zeros(l * m,dtype=<span class="string">'int'</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        b = np.array(list(np.binary_repr(np.abs(c[i]))),dtype=<span class="string">'int'</span>)</span><br><span class="line">        <span class="keyword">if</span>(c[i] &lt; <span class="number">0</span>):</span><br><span class="line">            b *= <span class="number">-1</span></span><br><span class="line">        c_star[(i * l) + (l-len(b)): (i+<span class="number">1</span>) * l] += b</span><br><span class="line">    <span class="keyword">return</span> c_star</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">switch_key</span><span class="params">(c,S,m,n,T)</span>:</span></span><br><span class="line">    l = int(np.ceil(np.log2(np.max(np.abs(c)))))</span><br><span class="line">    c_star = get_c_star(c,m,l)</span><br><span class="line">    S_star = get_S_star(S,m,n,l)</span><br><span class="line">    n_prime = n + <span class="number">1</span></span><br><span class="line">    S_prime = np.concatenate((np.eye(m),T.T),<span class="number">0</span>).T</span><br><span class="line">    A = (np.random.rand(n_prime - m, n*l) * <span class="number">10</span>).astype(<span class="string">'int'</span>)</span><br><span class="line">    E = (<span class="number">1</span> * np.random.rand(S_star.shape[<span class="number">0</span>],S_star.shape[<span class="number">1</span>])).astype(<span class="string">'int'</span>)</span><br><span class="line">    M = np.concatenate(((S_star - T.dot(A) + E),A),<span class="number">0</span>)</span><br><span class="line">    c_prime = M.dot(c_star)</span><br><span class="line">    <span class="keyword">return</span> c_prime,S_prime</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_S_star</span><span class="params">(S,m,n,l)</span>:</span></span><br><span class="line">    S_star = list()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(l):</span><br><span class="line">        S_star.append(S*<span class="number">2</span>**(l-i<span class="number">-1</span>))</span><br><span class="line">    S_star = np.array(S_star).transpose(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>).reshape(m,n*l)</span><br><span class="line">    <span class="keyword">return</span> S_star</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_T</span><span class="params">(n)</span>:</span></span><br><span class="line">    n_prime = n + <span class="number">1</span></span><br><span class="line">    T = (<span class="number">10</span> * np.random.rand(n,n_prime - n)).astype(<span class="string">'int'</span>)</span><br><span class="line">    <span class="keyword">return</span> T</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encrypt_via_switch</span><span class="params">(x,w,m,n,T)</span>:</span></span><br><span class="line">    c,S = switch_key(x*w,np.eye(m),m,n,T)</span><br><span class="line">    <span class="keyword">return</span> c,S</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>])</span><br><span class="line">m = len(x)</span><br><span class="line">n = m</span><br><span class="line">w = <span class="number">16</span></span><br><span class="line">S = generate_key(w,m,n)</span><br><span class="line"></span><br><span class="line">y = np.array([<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>])</span><br><span class="line">m = len(y)</span><br><span class="line">n = m</span><br><span class="line">w = <span class="number">16</span></span><br><span class="line">S = generate_key(w,m,n)</span><br><span class="line"></span><br><span class="line">print(x * <span class="number">10</span>)</span><br><span class="line">print(x + y)</span><br></pre></td></tr></table></figure>
<p>[ 0 10 30 50]<br>[3 4 6 8]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">T = get_T(n)</span><br><span class="line">cx,S = encrypt_via_switch(x,w,m,n,T)</span><br><span class="line">print(cx)</span><br><span class="line"><span class="comment">#T = get_T(n)</span></span><br><span class="line">cy,S = encrypt_via_switch(y,w,m,n,T)</span><br><span class="line">print(cy)</span><br></pre></td></tr></table></figure>
<p>[-115. -122.  -21.  -35.   23.]<br>[-177. -222.  -87. -177.   45.]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(decrypt(cx,S,w))</span><br><span class="line">print(decrypt(cy,S,w))</span><br></pre></td></tr></table></figure>
<p>[0 1 3 5]<br>[3 3 3 3]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">decrypt(cx * <span class="number">10</span>,S,w)</span><br></pre></td></tr></table></figure>
<p>array([ 0, 10, 30, 50])</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">decrypt(cx + cy,S,w)</span><br></pre></td></tr></table></figure>
<p>array([3, 4, 6, 8])</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://loyf.github.io/2019/05/30/同态加密Homomorphic-Encryption简介/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Loy Fan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Loy With No Title">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/30/同态加密Homomorphic-Encryption简介/" itemprop="url">同态加密Homomorphic Encryption简介</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-30T15:35:22+08:00">
                2019-05-30
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Cryptography/" itemprop="url" rel="index">
                    <span itemprop="name">Cryptography</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<p>首先，同态加密不是某种特定的加密算法，而是一种加密形式。</p>
<p><strong>同态加密允许人们对密文进行特定形式的代数运算得到仍然是加密的结果，将其解密所得到的结果与对明文进行同样的运算结果一样。</strong></p>
<p>换言之，这项技术令人们可以在加密的数据中进行诸如检索、比较等操作，得出正确的结果，而在整个处理过程中无需对数据进行解密。其意义在于，真正从根本上解决将数据及其操作委托给第三方时的保密问题，例如对于各种云计算的应用。这一直是密码学领域的一个重要课题。</p>
<p><strong>同态分类</strong><br>a) 如果满足 f(A)+f(B)=f(A+B)， 我们将这种加密函数叫做加法同态<br>b) 如果满足 f(A)×f(B)=f(A×B)， 我们将这种加密函数叫做乘法同态。<br>如果一个加密函数f只满足加法同态，就只能进行加减法运算；<br>如果一个加密函数f只满足乘法同态，就只能进行乘除法运算;</p>
<p><strong>全同态加密算法 Full Homomorphic Encryption</strong><br>如果一个加密函数同时满足加法同态和乘法同态，称为全同态加密。那么可以使用这个加密函数完成各种加密后的运算(加减乘除、多项式求值、指数、对数、三角函数)。</p>
<p>第一个满足加法和乘法同态的同态加密方法：2009年9月克雷格·金特里（Craig Gentry）的论文<a href="https://www.cs.cmu.edu/~odonnell/hits09/gentry-homomorphic-encryption.pdf" target="_blank" rel="noopener">Fully Homomorphic Encryption Using Ideal Lattices</a>从数学上提出了“全同态加密”的可行方法，即可以在不解密的条件下对加密数据进行任何可以在明文上进行的运算，使这项技术获取了决定性的突破。</p>
<p><strong>同态加密举例</strong><br>RSA 算法——乘法同态。<br>Elgamal 算法——乘法同态。<br>Paillier 算法——加法同态。<br>Gentry算法——全同态。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://loyf.github.io/2019/05/30/基于Keras实现加密过得数据的卷积神经网络/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Loy Fan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Loy With No Title">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/30/基于Keras实现加密过得数据的卷积神经网络/" itemprop="url">基于Keras实现加密过得数据的卷积神经网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-30T15:33:01+08:00">
                2019-05-30
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning-Cryptography/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning, Cryptography</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<blockquote>
<p>来自奥胡斯大学密码学PhD、Datadog机器学习工程师Morten Dahl介绍了如何实现基于加密数据进行训练和预测的卷积神经网络。本文进行概括和总结</p>
</blockquote>
<p><strong>工作</strong><br>使用一个经典CNN模型，使其能够用于基于加密数据进行训练和预测。</p>
<p><strong>动机</strong><br>CNN目前可以用于很多分析图像的领域。如果能够让实际使用的用户在应用场景下提供更多的数据，那么模型的性能肯定可以提高。但是这涉及很多用户的隐私问题，如果用户可以明确自己提交的数据是确保隐私安全的，那么将可以吸引更多用户使用这些工具，提交更多数据。<br>基于<a href="(https://mp.weixin.qq.com/s?__biz=MzI3ODkxODU3Mg==&amp;mid=2247485858&amp;idx=1&amp;sn=16767a8f029ba847ad48845a280211eb&amp;chksm=eb4eed61dc396477926f5d12265c1c3daf6f1c68fd4f82d340ea0692b7337e2feaa07b362406&amp;scene=21#wechat_redirect">MPC</a>)我们可以潜在地降低暴露信息的风险，从而增强参与的动机。更具体地说，通过转而在加密数据上训练，我们不仅可以防止任何人查看个人数据，还可以防止泄露学习到的模型参数。</p>
<p><strong>使用简单的CNN模型</strong><br>用例是经典的MNIST手写数字识别，模型是Keras示例中的CNN模型<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#keras</span></span><br><span class="line">feature_layers = [</span><br><span class="line">    Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)),</span><br><span class="line">    Activation(<span class="string">'relu'</span>),</span><br><span class="line">    Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>),</span><br><span class="line">    Activation(<span class="string">'relu'</span>),</span><br><span class="line">    MaxPooling2D(pool_size=(<span class="number">2</span>,<span class="number">2</span>)),</span><br><span class="line">    Dropout(<span class="number">.25</span>),</span><br><span class="line">    Flatten()</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">classification_layers = [</span><br><span class="line">    Dense(<span class="number">128</span>),</span><br><span class="line">    Activation(<span class="string">'relu'</span>),</span><br><span class="line">    Dropout(<span class="number">.50</span>),</span><br><span class="line">    Dense(NUM_CLASSES),</span><br><span class="line">    Activation(<span class="string">'softmax'</span>)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">model = Sequential(feature_layers + classification_layers)</span><br><span class="line">model.compile( loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">1</span>, batch_size=<span class="number">32</span>, verbose=<span class="number">1</span>, validation_data=(x_test, y_test))</span><br></pre></td></tr></table></figure></p>
<p>基本的想法是首先让图像通过一组特征层（feature layer），将输入图像的原始像素转换为和我们的分类任务更相关的抽象属性。接着通过一组分类层（classification layer）组合这些属性，以生成可能数字的概率分布。最终输出通常直接是概率最高的数字。使用Keras的优势是我们可以快速地在未加密数据上进行试验。</p>
<p><strong>基于SPDZ的安全计算</strong><br>CNN就绪后，我们接着来看MPC。我们将使用当前最先进的SPDZ协议，因为它允许我们只使用两个服务器，也允许我们通过将特定计算转移到离线阶段以改善在线表现。<br>和其他典型的安全计算协议一样，所有计算在一个域中进行，此处的域由一个<strong>质数Q</strong>表示。这意味着我们需要编码CNN使用的浮点数为以一个质数为模的整数，这给Q带来一些限制，进而对性能有所影响。<br>此外，在SPDZ协议这样的交互计算中，在典型的时间复杂度之外，同时还要考虑通讯和回合复杂度。通讯复杂度衡量在网络中发送的字节数，一个相对较慢的过程。回合复杂度衡量两个服务器之间的同步点数目，同步点可能阻塞其中一个服务器，使其无所事事，直到另一个服务器赶上来为止。因而两者均对总执行时间有很大的影响。<br>然而更重要的是，这些协议的“原生”操作只有加法和乘法。除法、比较等可以完成，但就它们的三项复杂度而言，要更昂贵。之后我们将看下如何缓解这引起的其中一些问题，而这里我们首先讨论基本的SPDZ协议。</p>
<p><strong>张量操作</strong><br>下面的代码为SPDZ协议实现$PublicTensor$和$PrivateTensor$两个类，分别代表两个服务器知道明文的张量和仅仅知道其秘密分享形式的加密值。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PrivateTensor</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, values, shares0=None, shares1=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> values <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            shares0, shares1 = share(values)</span><br><span class="line">        self.shares0 = shares0</span><br><span class="line">        self.shares1 = shares1</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reconstruct</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> PublicTensor(reconstruct(self.shares0, self.shares1))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x, y)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> type(y) <span class="keyword">is</span> PublicTensor:</span><br><span class="line">            shares0 = (x.values + y.shares0) % Q</span><br><span class="line">            shares1 =             y.shares1</span><br><span class="line">            <span class="keyword">return</span> PrivateTensor(<span class="keyword">None</span>, shares0, shares1)</span><br><span class="line">        <span class="keyword">if</span> type(y) <span class="keyword">is</span> PrivateTensor:</span><br><span class="line">            shares0 = (x.shares0 + y.shares0) % Q</span><br><span class="line">            shares1 = (x.shares1 + y.shares1) % Q</span><br><span class="line">            <span class="keyword">return</span> PrivateTensor(<span class="keyword">None</span>, shares0, shares1)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mul</span><span class="params">(x, y)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> type(y) <span class="keyword">is</span> PublicTensor:</span><br><span class="line">            shares0 = (x.shares0 * y.values) % Q</span><br><span class="line">            shares1 = (x.shares1 * y.values) % Q</span><br><span class="line">            <span class="keyword">return</span> PrivateTensor(<span class="keyword">None</span>, shares0, shares1)</span><br><span class="line">        <span class="keyword">if</span> type(y) <span class="keyword">is</span> PrivateTensor:</span><br><span class="line">            a, b, a_mul_b = generate_mul_triple(x.shape, y.shape)</span><br><span class="line">            alpha = (x - a).reconstruct()</span><br><span class="line">            beta  = (y - b).reconstruct()</span><br><span class="line">            <span class="keyword">return</span> alpha.mul(beta) + \</span><br><span class="line">                   alpha.mul(b) + \</span><br><span class="line">                   a.mul(beta) + \</span><br><span class="line">                   a_mul_b</span><br></pre></td></tr></table></figure>
<p>以上代码用到的工具函数<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">share</span><span class="params">(secrets)</span>:</span></span><br><span class="line">    shares0 = sample_random_tensor(secrets.shape)</span><br><span class="line">    shares1 = (secrets - shares0) % Q</span><br><span class="line">    <span class="keyword">return</span> shares0, shares1</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reconstruct</span><span class="params">(shares0, shares1)</span>:</span></span><br><span class="line">    secrets = (shares0 + shares1) % Q</span><br><span class="line">    <span class="keyword">return</span> secrets</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_mul_triple</span><span class="params">(x_shape, y_shape)</span>:</span></span><br><span class="line">    a = sample_random_tensor(x_shape)</span><br><span class="line">    b = sample_random_tensor(y_shape)</span><br><span class="line">    c = np.multiply(a, b) % Q</span><br><span class="line">    <span class="keyword">return</span> PrivateTensor(a), PrivateTensor(b), PrivateTensor(c)</span><br></pre></td></tr></table></figure></p>
<p><strong>适配模型</strong><br>虽然原则上基于我们现有的模型安全地计算任何函数是可能的，实践中需要做的是先考虑对MPC更友好的模型变体，以及对模型更友好的加密协议。用稍微形象一点的话说，我们经常需要打开两个黑箱，让两个技术更好地适配彼此。<br>这一做法的根源在于，加密操作下，有一些操作惊人地昂贵。我们之前提到过，加法和乘法相对廉价，而比较和基于私密分母的除法则不然。基于这一原因，我们对模型做了一些改动，以避免这一问题（代价过高）。</p>
<p><strong>优化器</strong><br>首先涉及的是优化器：尽管许多实现基于Adam的高效而选择了它，Adam涉及对私密值取平方根，以及在除法中使用私密值作分母。尽管理论上安全地进行这些计算是可能的，在实践中它会是性能的显著瓶颈，因此需要避免使用Adam。<br>一个简单的补救方案是转而使用动量SGD（momentum SGD）优化器，它可能意味着较长的训练时间，但只使用简单的操作。<br>改为：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile( loss=<span class="string">'categorical_crossentropy'</span>, optimizer=SGD(clipnorm=<span class="number">10000</span>, clipvalue=<span class="number">10000</span>), metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure></p>
<p><strong>网络层</strong><br>ReLU和最大池化层有“比较”操作的问题，使用了高阶sigmoid激活函数和平均池化层代替。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Activation('relu')</span></span><br><span class="line">Activation(<span class="string">'sigmoid'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#MaxPooling2D(pool_size=(2,2))</span></span><br><span class="line">AveragePooling2D(pool_size=(<span class="number">2</span>,<span class="number">2</span>))</span><br></pre></td></tr></table></figure></p>
<p>另外就是要提高epoch数量，epochs=15。剩下的层很好处理。dropout和平层（flatten）不在乎是加密设定还是非加密设定，密集层和卷积层是矩阵点积，只需要基本操作。&gt; 来自奥胡斯大学密码学PhD、Datadog机器学习工程师Morten Dahl介绍了如何实现基于加密数据进行训练和预测的卷积神经网络。本文进行概括和总结</p>
<p><strong>工作</strong><br>使用一个经典CNN模型，使其能够用于基于加密数据进行训练和预测。</p>
<p><strong>动机</strong><br>CNN目前可以用于很多分析图像的领域。如果能够让实际使用的用户在应用场景下提供更多的数据，那么模型的性能肯定可以提高。但是这涉及很多用户的隐私问题，如果用户可以明确自己提交的数据是确保隐私安全的，那么将可以吸引更多用户使用这些工具，提交更多数据。<br>基于<a href="(https://mp.weixin.qq.com/s?__biz=MzI3ODkxODU3Mg==&amp;mid=2247485858&amp;idx=1&amp;sn=16767a8f029ba847ad48845a280211eb&amp;chksm=eb4eed61dc396477926f5d12265c1c3daf6f1c68fd4f82d340ea0692b7337e2feaa07b362406&amp;scene=21#wechat_redirect">MPC</a>)我们可以潜在地降低暴露信息的风险，从而增强参与的动机。更具体地说，通过转而在加密数据上训练，我们不仅可以防止任何人查看个人数据，还可以防止泄露学习到的模型参数。</p>
<p><strong>使用简单的CNN模型</strong><br>用例是经典的MNIST手写数字识别，模型是Keras示例中的CNN模型<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#keras</span></span><br><span class="line">feature_layers = [</span><br><span class="line">    Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)),</span><br><span class="line">    Activation(<span class="string">'relu'</span>),</span><br><span class="line">    Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), padding=<span class="string">'same'</span>),</span><br><span class="line">    Activation(<span class="string">'relu'</span>),</span><br><span class="line">    MaxPooling2D(pool_size=(<span class="number">2</span>,<span class="number">2</span>)),</span><br><span class="line">    Dropout(<span class="number">.25</span>),</span><br><span class="line">    Flatten()</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">classification_layers = [</span><br><span class="line">    Dense(<span class="number">128</span>),</span><br><span class="line">    Activation(<span class="string">'relu'</span>),</span><br><span class="line">    Dropout(<span class="number">.50</span>),</span><br><span class="line">    Dense(NUM_CLASSES),</span><br><span class="line">    Activation(<span class="string">'softmax'</span>)</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">model = Sequential(feature_layers + classification_layers)</span><br><span class="line">model.compile( loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(x_train, y_train, epochs=<span class="number">1</span>, batch_size=<span class="number">32</span>, verbose=<span class="number">1</span>, validation_data=(x_test, y_test))</span><br></pre></td></tr></table></figure></p>
<p>基本的想法是首先让图像通过一组特征层（feature layer），将输入图像的原始像素转换为和我们的分类任务更相关的抽象属性。接着通过一组分类层（classification layer）组合这些属性，以生成可能数字的概率分布。最终输出通常直接是概率最高的数字。使用Keras的优势是我们可以快速地在未加密数据上进行试验。</p>
<p><strong>基于SPDZ的安全计算</strong><br>CNN就绪后，我们接着来看MPC。我们将使用当前最先进的SPDZ协议，因为它允许我们只使用两个服务器，也允许我们通过将特定计算转移到离线阶段以改善在线表现。<br>和其他典型的安全计算协议一样，所有计算在一个域中进行，此处的域由一个<strong>质数Q</strong>表示。这意味着我们需要编码CNN使用的浮点数为以一个质数为模的整数，这给Q带来一些限制，进而对性能有所影响。<br>此外，在SPDZ协议这样的交互计算中，在典型的时间复杂度之外，同时还要考虑通讯和回合复杂度。通讯复杂度衡量在网络中发送的字节数，一个相对较慢的过程。回合复杂度衡量两个服务器之间的同步点数目，同步点可能阻塞其中一个服务器，使其无所事事，直到另一个服务器赶上来为止。因而两者均对总执行时间有很大的影响。<br>然而更重要的是，这些协议的“原生”操作只有加法和乘法。除法、比较等可以完成，但就它们的三项复杂度而言，要更昂贵。之后我们将看下如何缓解这引起的其中一些问题，而这里我们首先讨论基本的SPDZ协议。</p>
<p><strong>张量操作</strong><br>下面的代码为SPDZ协议实现$PublicTensor$和$PrivateTensor$两个类，分别代表两个服务器知道明文的张量和仅仅知道其秘密分享形式的加密值。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PrivateTensor</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, values, shares0=None, shares1=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> values <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            shares0, shares1 = share(values)</span><br><span class="line">        self.shares0 = shares0</span><br><span class="line">        self.shares1 = shares1</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reconstruct</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> PublicTensor(reconstruct(self.shares0, self.shares1))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(x, y)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> type(y) <span class="keyword">is</span> PublicTensor:</span><br><span class="line">            shares0 = (x.values + y.shares0) % Q</span><br><span class="line">            shares1 =             y.shares1</span><br><span class="line">            <span class="keyword">return</span> PrivateTensor(<span class="keyword">None</span>, shares0, shares1)</span><br><span class="line">        <span class="keyword">if</span> type(y) <span class="keyword">is</span> PrivateTensor:</span><br><span class="line">            shares0 = (x.shares0 + y.shares0) % Q</span><br><span class="line">            shares1 = (x.shares1 + y.shares1) % Q</span><br><span class="line">            <span class="keyword">return</span> PrivateTensor(<span class="keyword">None</span>, shares0, shares1)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mul</span><span class="params">(x, y)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> type(y) <span class="keyword">is</span> PublicTensor:</span><br><span class="line">            shares0 = (x.shares0 * y.values) % Q</span><br><span class="line">            shares1 = (x.shares1 * y.values) % Q</span><br><span class="line">            <span class="keyword">return</span> PrivateTensor(<span class="keyword">None</span>, shares0, shares1)</span><br><span class="line">        <span class="keyword">if</span> type(y) <span class="keyword">is</span> PrivateTensor:</span><br><span class="line">            a, b, a_mul_b = generate_mul_triple(x.shape, y.shape)</span><br><span class="line">            alpha = (x - a).reconstruct()</span><br><span class="line">            beta  = (y - b).reconstruct()</span><br><span class="line">            <span class="keyword">return</span> alpha.mul(beta) + \</span><br><span class="line">                   alpha.mul(b) + \</span><br><span class="line">                   a.mul(beta) + \</span><br><span class="line">                   a_mul_b</span><br></pre></td></tr></table></figure>
<p>以上代码用到的工具函数<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">share</span><span class="params">(secrets)</span>:</span></span><br><span class="line">    shares0 = sample_random_tensor(secrets.shape)</span><br><span class="line">    shares1 = (secrets - shares0) % Q</span><br><span class="line">    <span class="keyword">return</span> shares0, shares1</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reconstruct</span><span class="params">(shares0, shares1)</span>:</span></span><br><span class="line">    secrets = (shares0 + shares1) % Q</span><br><span class="line">    <span class="keyword">return</span> secrets</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_mul_triple</span><span class="params">(x_shape, y_shape)</span>:</span></span><br><span class="line">    a = sample_random_tensor(x_shape)</span><br><span class="line">    b = sample_random_tensor(y_shape)</span><br><span class="line">    c = np.multiply(a, b) % Q</span><br><span class="line">    <span class="keyword">return</span> PrivateTensor(a), PrivateTensor(b), PrivateTensor(c)</span><br></pre></td></tr></table></figure></p>
<p><strong>适配模型</strong><br>虽然原则上基于我们现有的模型安全地计算任何函数是可能的，实践中需要做的是先考虑对MPC更友好的模型变体，以及对模型更友好的加密协议。用稍微形象一点的话说，我们经常需要打开两个黑箱，让两个技术更好地适配彼此。<br>这一做法的根源在于，加密操作下，有一些操作惊人地昂贵。我们之前提到过，加法和乘法相对廉价，而比较和基于私密分母的除法则不然。基于这一原因，我们对模型做了一些改动，以避免这一问题（代价过高）。</p>
<p><strong>优化器</strong><br>首先涉及的是优化器：尽管许多实现基于Adam的高效而选择了它，Adam涉及对私密值取平方根，以及在除法中使用私密值作分母。尽管理论上安全地进行这些计算是可能的，在实践中它会是性能的显著瓶颈，因此需要避免使用Adam。<br>一个简单的补救方案是转而使用动量SGD（momentum SGD）优化器，它可能意味着较长的训练时间，但只使用简单的操作。<br>改为：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile( loss=<span class="string">'categorical_crossentropy'</span>, optimizer=SGD(clipnorm=<span class="number">10000</span>, clipvalue=<span class="number">10000</span>), metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure></p>
<p><strong>网络层</strong><br>ReLU和最大池化层有“比较”操作的问题，使用了高阶sigmoid激活函数和平均池化层代替。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Activation('relu')</span></span><br><span class="line">Activation(<span class="string">'sigmoid'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#MaxPooling2D(pool_size=(2,2))</span></span><br><span class="line">AveragePooling2D(pool_size=(<span class="number">2</span>,<span class="number">2</span>))</span><br></pre></td></tr></table></figure></p>
<p>另外就是要提高epoch数量，epochs=15。剩下的层很好处理。dropout和平层（flatten）不在乎是加密设定还是非加密设定，密集层和卷积层是矩阵点积，只需要基本操作。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://loyf.github.io/2019/04/29/通往真理的最短路径-复数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Loy Fan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Loy With No Title">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/29/通往真理的最短路径-复数/" itemprop="url">通往真理的最短路径 - 复数</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-29T19:24:20+08:00">
                2019-04-29
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Math/" itemprop="url" rel="index">
                    <span itemprop="name">Math</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<p>mathjax:true</p>
<p><center><br><img src="https://img-blog.csdnimg.cn/20190429155508364.png" width="60%" alt></center></p>
<blockquote>
<p>在实数域中，连接两个真理的最短的路径是通过复数域    ——雅克·阿达马</p>
</blockquote>
<h1 id="复数认知阶段"><a href="#复数认知阶段" class="headerlink" title="复数认知阶段"></a>复数认知阶段</h1><p><strong>阶段一</strong></p>
<p>高中数学定义</p>
<script type="math/tex; mode=display">
i=\sqrt{-1}</script><p>再定义复数</p>
<script type="math/tex; mode=display">
a+b i \quad(a, b \in \mathbb{R})</script><p>这样一元二次方程就总是有解了！</p>
<script type="math/tex; mode=display">
a x^{2}+b x+c=0 \quad(a \neq 0)</script><script type="math/tex; mode=display">
x=\frac{-b \pm \sqrt{b^{2}-4 a c}}{2 a}</script><p>然后就可以出很多乱七八糟的考题了！</p>
<p><strong>阶段二</strong></p>
<p>有了复数之后，以下原本在实数上不可以进行的操作都可以进行</p>
<ul>
<li>任意开根号<script type="math/tex; mode=display">
\sqrt{-4}=2 i</script></li>
<li>对负数做对数运算<script type="math/tex; mode=display">
\ln (-5)=\ln \left(5 e^{i \pi}\right)=\ln 5+\ln e^{i \pi}=\ln 5+i \pi</script></li>
</ul>
<p>但这两种还是不可以</p>
<ul>
<li>除以0</li>
<li>$\log (0)$</li>
</ul>
<p><strong>扩展数系</strong></p>
<p>问：扩展数系的时候，为什么不能发明一种数系，兼容“除以0”这个操作呢？<br>答：因为没有办法自洽，兼容“除以0”之后会得到悖论。所以“兼容除以0”这个目标就是错误的。就像“永动机”这个目标一样。</p>
<script type="math/tex; mode=display">
0=0 \Longrightarrow 2 \cdot 0=1 \cdot 0 \Longrightarrow \frac{2 \cdot 0}{0}=\frac{1 \cdot 0}{0} \Longrightarrow 2=1</script><p>问：扩展数系的时候，为什么可以轻松得到$i=\sqrt{-1}$？<br>答：因为复数这个东西本来就是合理的，等待人们发现而已。（有一种轮回的感觉。。）</p>
<h1 id="复数是二维的数"><a href="#复数是二维的数" class="headerlink" title="复数是二维的数"></a>复数是二维的数</h1><p><a href="https://www.youtube.com/watch?v=xnJPy-lsZKQ" target="_blank" rel="noopener">理解多维空间的一个小视频 - YouTube</a></p>
<p>假设有一个生活在二维空间中的纸片人：<br><img src="https://img-blog.csdnimg.cn/20190429190136734.png" width="50%" alt><br>突然发现有一个黑点在草地上忽大忽小的闪烁，纸片人完全不知道怎么去解释：<br><img src="https://img-blog.csdnimg.cn/20190429190242473.png" width="50%" alt><br><img src="https://img-blog.csdnimg.cn/20190429190334418.png" width="50%" alt><br>如果切换到三维视角去的话，问题就很简单了，原来是一个三维的球体穿过二维平面：<br><img src="https://img-blog.csdnimg.cn/20190429190408289.png" width="50%" alt><br>实数是一维的数，既生活在一维的实数轴上，又困囿其上：<br><img src="https://img-blog.csdnimg.cn/20190429185223317.png" width="50%" alt></p>
<p>而复数生活在二维复平面，拥有更大的自由度：<br><img src="https://img-blog.csdnimg.cn/20190429185325394.png" width="50%" alt></p>
<h1 id="复数的历史"><a href="#复数的历史" class="headerlink" title="复数的历史"></a>复数的历史</h1><p><strong>纸片人卡尔达诺</strong><br><img src="https://img-blog.csdnimg.cn/20190429185712690.png" width="20%" alt></p>
<p>意大利数学家，吉罗拉莫·卡尔达诺（1501－1576），在它的著作《大术》中（这本书首次记载了一元三次方程的完整解法）提到这个一个问题，能否把10分成两部分，使它们的乘积为40？</p>
<p>他给出一个答案，令：</p>
<script type="math/tex; mode=display">
a=5+\sqrt{-15}\quad b=5-\sqrt{-15}</script><p>这样就满足题目的要求：</p>
<script type="math/tex; mode=display">
a+b=10\quad a\cdot b=40</script><p>不过他自己也认为这不过就是一个数学游戏，虽然出现了虚数，但是“既不可捉摸又没有什么用处”。</p>
<p>此时的卡尔达诺就好像之前的纸片人，虽然想到了虚数，触摸到了更高的维度，但是终究还是把它看成一种幻想。</p>
<p>之后的笛卡尔把$i=\sqrt{-1}$称为虚数，也就是虚幻的、想像出来的数；莱布尼兹描述它为“介乎于存在与不存在之间的两栖数”。</p>
<p>确实，纸片人要跳出自己的维度去想问题是非常困难的。</p>
<p><strong>邦贝利的思维飞跃</strong><br>拉斐尔·邦贝利（1526－1572），文艺复兴时期欧洲著名的工程师，同时也是一个卓越的数学家，其出版于1572年的《代数学》一书讨论了负数的平方根（虚数）：<br><img src="https://img-blog.csdnimg.cn/20190429190651830.png" width="30%" alt><br>正是这本书产生了一个思维飞跃。</p>
<ul>
<li><p>标准的一元二次方程</p>
<script type="math/tex; mode=display">
a x^{2}+b x+c=0 \quad(a \neq 0)</script><p>在$b^{2}-4ac &lt; 0$时曲线与x轴不相交，应该无解。</p>
</li>
<li><p>一元三次方程</p>
<script type="math/tex; mode=display">
x^{3}-3 p x-2 q=0</script><p>通解是</p>
<script type="math/tex; mode=display">
x=\sqrt[8]{q+\sqrt{q^{2}-p^{3}}}+\sqrt[3]{q-\sqrt{q^{2}-p^{3}}}</script><p>假设方程</p>
<script type="math/tex; mode=display">
x^{3}-15 x-4=0</script><p>图像是<br><center><br><img src="https://img-blog.csdnimg.cn/20190429191052637.png" width="50%" alt></center></p>
</li>
</ul>
<p>套用通解得到</p>
<script type="math/tex; mode=display">
x=\sqrt[3]{2+\sqrt{2^{2}-5^{3}}}+\sqrt[3]{2-\sqrt{2^{2}-5^{3}}}=\sqrt[3]{2+11 i}+\sqrt[3]{2-11 i}</script><p>邦贝利指出：从几何上看是有解的，但是必须通过虚数来求解！<br>邦贝利大胆地定义了复数的乘法（就是多项式乘法的合理延伸）：</p>
<script type="math/tex; mode=display">
(a+bi)(c+di)=ac+(ad+bc)i+bdi^2</script><p>最终通过复数以及复数乘法，邦贝利解出了此方程的三个实数解。</p>
<p>这是一个巨大的思维飞跃，就好像刚才的纸片小人，困惑于“为什么有一个黑点在草地上忽大忽小的闪烁”？最终发现，需要通过更高纬度才能真正解决这个问题。邦贝利通过更高维度的复平面，解决了低维度的实数问题，真正的把复数带入了人们的视野。所以他被认为是复数的发现者。</p>
<h1 id="更高维的数"><a href="#更高维的数" class="headerlink" title="更高维的数"></a>更高维的数</h1><p>自然会有这么一个问题，是否有更高维度的数？答案是有的，比如四元数。<br><img src="https://img-blog.csdnimg.cn/20190429192119991.png" width="20%" alt><br>威廉·哈密顿爵士（1805－1865）发现了四元数：</p>
<script type="math/tex; mode=display">
a+bi+cj+dk</script><p>其中i、j、k就是对虚数维度的扩展。为此还成立了四元数推广委员会，提议学校像实数一样教授四元数。<br>四元数刚开始的时候引起了很大的争议，计算很复杂，但是用处不明显。用处不明显的原因或许是，当时面临的问题还不够复杂，还用不到比复数还高的维度。到了现代，终于在电脑动画中、量子物理中找到了四元数更多的应用，只是这些应用对普通人距离太远了。</p>
<p><strong>参考</strong><br><a href="https://www.matongxue.com/madocs/1709/" target="_blank" rel="noopener">https://www.matongxue.com/madocs/1709/</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://loyf.github.io/2019/04/23/反向传播算法-基于维基百科的理解-Back-Propagation-Wikipedia/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Loy Fan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Loy With No Title">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/23/反向传播算法-基于维基百科的理解-Back-Propagation-Wikipedia/" itemprop="url">反向传播算法 基于维基百科的理解 | Back Propagation - Wikipedia</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-23T20:31:47+08:00">
                2019-04-23
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<p>mathjax: true</p>
<blockquote>
<p>维基百科上的中文版反向传播算法的翻译不是特别准确，英文原版的结构也不是很容易理解，于是结合了各方资料重新写了一个版本。以下是中英维基百科的链接。<br><a href="https://en.wikipedia.org/wiki/Backpropagation" target="_blank" rel="noopener">BackPropagation - Wikipedia</a><br><a href="https://www.bk.gugeeseo.com/baike-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95" target="_blank" rel="noopener">反向传播算法 - 维基百科中文版网站</a></p>
</blockquote>
<p><strong>反向传播</strong>（Backpropagation，缩写为BP）是一种用于人工神经网络的方法，用于计算在计算网络中使用的权重需要用到的梯度。反向传播是“误差反向传播”的简写，因为误差是在输出时计算的，并且从输出层往后分布于网络的各个层。它通常被用来训练深层神经网络。<br>反向传播是将<a href="https://en.wikipedia.org/wiki/Delta_rule" target="_blank" rel="noopener">delta规则</a>推广到多层前馈网络，通过使用<a href="https://en.wikipedia.org/wiki/Chain_rule" target="_blank" rel="noopener">链规则</a>迭代计算每个层的梯度来实现。它与<a href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm" target="_blank" rel="noopener">高斯-牛顿算法</a>密切相关，是神经反向传播研究（生物学上）的一部分。<br>反向传播是一种更通用的技术称作<a href="https://en.wikipedia.org/wiki/Automatic_differentiation" target="_blank" rel="noopener">自动微分</a>的特例。在学习的中，反向传播通常被<a href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank" rel="noopener">梯度下降</a>优化算法所使用，通过计算<a href="https://en.wikipedia.org/wiki/Loss_function" target="_blank" rel="noopener">损失函数</a>的梯度来更新神经元权重，以最小化损失函数。</p>
<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><p>任何<a href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" rel="noopener">监督学习</a>算法的目标都是找到一个将一组输入映射到其正确的输出的最佳函数，反向传播的动机是训练一个多层神经网络，使其能够学习适当的内部表达式，从而可以学习任何输入到输出的映射。</p>
<h1 id="直观理解"><a href="#直观理解" class="headerlink" title="直观理解"></a>直观理解</h1><p><strong>作为一个优化问题学习</strong></p>
<p>为了理解反向传播算法的数学推导，首先要对神经元的实际输出与特定训练示例的正确输出之间的关系进行一些直觉理解。考虑一个简单的神经网络，它有两个输入单元，一个输出单元，没有隐藏单元，每个神经元使用一个线性输出（这与实际神经网络上的大多数工作不同，实际中从输入到输出的映射基本都是非线性的），即其输入的加权和。（下图中 y 应改为 t ）<br><img src="https://img-blog.csdnimg.cn/20190422205909861.png" width="40%" alt><br>最初，在训练之前，权重会被随机设置，接着神经元从训练样本中学习，在这种情况下，训练样本由一组元组$(x_1,x_2,y)$组成，其中$x_1,x_2$是网络的输入，$y$是正确的输出（当网络被训练时，由这些输入而应该得到的输出）。这个最初的网络，在给定了$x_1,x_2$后，会计算出一个输出结果$t$，很可能会与$y$不同（因为给了随机权重）。度量预期输出$y$和实际输出$t$之间的误差常用的方法是计算它们的平方误差：</p>
<script type="math/tex; mode=display">
E=(t-y)^{2}</script><p>其中$E$就是误差。<br>这里给一个例子，我们考虑这个网络的训练集只有一个样本$(1,1,0)$，因此输入$x_1,x_2$分别为1和1，正确的输出为$y=0$。现在若将实际输出 t 画在 x 轴（图中标为y了，本文为了了统一符号，应改为 t ），误差 E 画在 y 轴，得出的是一条抛物线。抛物线的极小值对应输出 t ，最小化了误差 E 。对于单一训练实例，极小值还会接触到 x 轴，这意味着误差为零，网络可以产生与期望输出 y 完全匹配的输出 t。因此，把输入映射到输出的问题就化为了一个找到一个能产生最小误差的函数的最优化问题。<br><img src="https://img-blog.csdnimg.cn/20190423130446545.png" width="40%" alt><br>然而，一个神经元的输出取决于其所有输入的加权总和：</p>
<script type="math/tex; mode=display">
t=x_{1} w_{1}+x_{2} w_{2}</script><p>其中$w_1,w_2$是从输入单元到输出单元的权重。因此，误差取决于出入到该神经元的权重，也是网络要学习最终需要改变的。若每个权重都画在一个水平的轴上，而误差画在垂直轴上，得出的就是一个抛物面（若一个神经元有 k 个权重，则误差曲面的维度就会是 k+1，因而就是二维抛物线的 k+1 维等价）。<br><img src="https://img-blog.csdnimg.cn/20190423142141282.png" width="50%" alt><br>反向传播算法的目的是找到一组能最大限度地减小误差的权重。寻找抛物线或任意维度中的任何函数的极大值的方法有若干种。第一种方法是求解方程组，但这仅适用于线性系统的网络，然而我们需要的是可以训练多层非线性网络的方法（因为多层线性网络与单层网络等价）。第二种方法，也是在反向传播中使用的方法是梯度下降法。</p>
<p><strong>运用类比理解梯度下降法</strong></p>
<p>梯度下降法背后的直观感受可以用假设情境进行说明。一个被卡在山上的人正在试图下山（即试图找到极小值）。大雾使得能见度非常低。因此，下山的道路是看不见的，所以他必须利用局部信息来找到极小值。他可以使用梯度下降法，该方法涉及到察看在他当前位置山的陡峭程度，然后沿着负陡度（即下坡）最大的方向前进。如果他要找到山顶（即极大值）的话，他需要沿着正陡度（即上坡）最大的方向前进。使用此方法，他会最终找到下山的路。不过，要假设山的陡度不能通过简单地观察得到，而需要复杂的工具测量，而这个工具此人恰好有。需要相当长的一段时间用仪器测量山的陡峭度，因此如果他想在日落之前下山，就需要最小化仪器的使用率。问题就在于怎样选取他测量山的陡峭度的频率才不致偏离路线。<br>在这个类比中，此人代表反向传播算法，而下山路径表示能使误差最小化的权重集合。山的陡度表示误差曲面在该点的斜率。他要前行的方向对应于误差曲面在该点的梯度。用来测量陡峭度的工具是微分（误差曲面的斜率可以通过对平方误差函数在该点求导数计算出来）。他在两次测量之间前行的距离（与测量频率成正比）是算法的学习速率。参见限制一节中对此类型“爬山”算法的限制的讨论。</p>
<h1 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h1><p>由于反向传播使用梯度下降法，需要计算平方误差函数对网络权值的导数。假设对于一个输出神经元，平方误差函数为：</p>
<script type="math/tex; mode=display">
E=\frac{1}{2}(t-y)^{2}</script><p>其中，$E$为平方误差，$t$为训练样本的目标输出，$y$为输出神经元的实际输出。<br>加入系数1/2是为了抵消微分出来的指数。之后，该表达式会乘以一个任意的学习率，因此常系数无关紧要。对于每个神经元$j$，它的输出$a_j$定义为</p>
<script type="math/tex; mode=display">
a_{j}=\varphi\left(\text { net }_{j}\right)=\varphi\left(\sum_{i=1}^{n} w_{i j} a_{i}\right)</script><p>通向一个神经元的输入$net_j$是之前神经元输出$a_j$的加权和。若该神经元处于输入层后的第一层，输入层的输出$a_i$就是网络的输入$x_i$。该神经元的输入数量是$n$。变量$w_{ij}$表示神经元$i,j$之间的权值。<br>激活函数$\varphi$一般是非线性可微函数。常用的激活函数是sigmoid函数</p>
<script type="math/tex; mode=display">
\varphi(z)=\frac{1}{1+e^{-z}}</script><p>其导数形式</p>
<script type="math/tex; mode=display">
\frac{\partial \varphi}{\partial z}=\varphi(1-\varphi)</script><blockquote>
<p>反向传播算法主要由两个阶段组成：激励传播与权值更新。</p>
</blockquote>
<p><strong>第一阶段：激励传播</strong><br>每次迭代中的传播环节包含两步：</p>
<ul>
<li>（向前传播阶段）将训练数据输入网络以获得激励响应（实际输出）。</li>
<li>（反向传播阶段）将激励响应同目标输出求差，从而获得输出层、隐藏层的响应误差。</li>
</ul>
<p>第一阶段即损失函数的计算，可以描述如下：</p>
<script type="math/tex; mode=display">
J\left(w_{0}, w_{1}, \ldots, w_{n}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{w}\left(x^{(i)}\right)-y^{(i)}\right)^{2}</script><p>其中$h_{w}\left(x^{(i)}\right)$即为$a_i$。</p>
<p><strong>第二阶段：权值更新</strong><br>对于每一个神经元上的权值，按照以下步骤进行更新：</p>
<ul>
<li>将输入和误差相乘，从而获得权重的梯度；</li>
<li>将这个梯度乘上一个比例并取反后加到权重上。<br>这个比例（学习率）将会影响到训练过程的速度和效果，因此成为“训练因子”。梯度的方向指明了误差扩大的方向，因此在更新权重的时候需要对其取反，从而减小权重引起的误差。</li>
</ul>
<p>第二阶段即使用梯度下降的权值更新公式，可以描述如下：</p>
<script type="math/tex; mode=display">
w_{j} :=w_{j}-\alpha \sum_{i=1}^{m}\left(h_{w}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}</script><p>$\alpha$即学习率，超参。</p>
<p><strong>第 1 和第 2 阶段可以反复循环迭代，直到网络对输入的响应达到满意的预定的目标范围为止。</strong></p>
<h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><p>BP算法的名称意味着误差会从输出结点反向传播到输入结点。严格地讲，反向传播算法对网络的可修改权值计算了网络误差的梯度。这个梯度会在简单随机梯度下降法中经常用来求最小化误差的权重。通常“反向传播”这个词使用更一般的含义，用来指涵盖了计算梯度以及在随机梯度下降法中使用的整个过程。在适用反向传播算法的网络中，它通常可以快速收敛到令人满意的极小值。</p>
<hr>
<p><strong>BP算法</strong></p>
<p>训练集    $\left\{\left(x^{(1)}, y^{(1)}\right), \ldots,\left(x^{(m)}, y^{(m)}\right)\right\}$</p>
<p>设    $\Delta_{i j}^{(l)}=0(\text { for all } l, i, j)$</p>
<p>$\begin{array}{l}{\text {For } i=1 \text { to } m}\end{array}$</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\text { Set } a^{(1)}=x^{(i)}} \\ {\text { Perform forward propagation to compute } a^{(l)} \text { for } l=2,3, \ldots, L} \\ {\text { Using } y^{(i)}, \text { compute } \delta^{(L)}=a^{(L)}-y^{(i)}} \\ {\text { Compute } \delta^{(L-1)}, \delta^{(l+1)}, \ldots, \delta^{(2)}} \\ {\Delta_{i j}^{(l)} :=\Delta_{i j}^{(l)}+a_{j}^{(l)} \delta_{i}^{(l+1)}}\end{array}</script><p>$\begin{array}{l}{D_{i j}^{(l)} :=\frac{1}{m} \Delta_{i j}^{(l)}+\lambda W_{i j}^{(l)}} &amp; {\text { if } j \neq 0} \\ {D_{i j}^{(l)} :=\frac{1}{m} \Delta_{i j}^{(l)}} &amp; {\text { if } j=0}\end{array}$</p>
<p>其中    $\frac{\partial}{\partial W_{i j}^{(l)}} J(W)=D_{i j}^{(l)}$</p>
<hr>
<p><strong>问题：什么是 $\delta_{j}^{(l)}$ ？</strong><br>对于l层的元素j的“误差”是：</p>
<script type="math/tex; mode=display">
\text { Intuition: } \delta_{j}^{(l)}=\text { "error" of node } j \text { in layer } l</script><p>假设层数为4，对于每一层的输出元素来说，误差是：</p>
<script type="math/tex; mode=display">
\delta_{j}^{(4)}={a_{j}^{(4)}-y_{j}}</script><script type="math/tex; mode=display">
\delta^{(3)}=\left(W^{(3)}\right)^{T} \delta^{(4)} \cdot * g^{\prime}\left(z^{(3)}\right)</script><script type="math/tex; mode=display">
\delta^{(2)}=\left(W^{(2)}\right)^{T} \delta^{(3)} \cdot * g^{\prime}\left(z^{(2)}\right)</script><p>其中$g^{\prime}$是激活函数的偏导数。</p>
<p><strong>问题：每一层的“误差”为什么是这么计算的？权值是如何更新的？</strong><br>==（推导过程——划重点）==<br>我们都知道，前馈神经网络可以由输出层的输出计算损失函数的值，得到误差，而梯度下降每一层都需要有明确的误差才能更新参数，因此反向传播的工作是将输出层的误差传递给隐藏层。<br>假设我们的网络只有一个隐藏层，如下图：<br><img src="https://img-blog.csdnimg.cn/20190423195004808.png" width="50%" alt><br>输出层的误差为$e_{o1},e_{o2}$，我们现在想要得到神经元c的误差，容易想到我们可以把输出神经元e,f的误差按照权值分配给c,d两个神经元，于是我们有神经元c和d的误差$e_{h1},e_{h2}$分别是($e_{h2}$图中未标出)：</p>
<script type="math/tex; mode=display">
e_{h 1}=\frac{w_{11}^{2}}{w_{11}^{2}+w_{21}^{2}} \cdot e_{o 1}+\frac{w_{12}^{2}}{w_{12}^{2}+w_{22}^{2}} \cdot e_{o 2}</script><script type="math/tex; mode=display">
e_{h 2}=\frac{w_{21}^{2}}{w_{11}^{2}+w_{21}^{2}} \cdot e_{o 1}+\frac{w_{22}^{2}}{w_{12}^{2}+w_{22}^{2}} \cdot e_{o 2}</script><p>转为矩阵表示：</p>
<script type="math/tex; mode=display">
\left( \begin{array}{c}{e_{h 1}} \\ {e_{h 2}}\end{array}\right)=\left( \begin{array}{cc}{\frac{w_{11}^{2}}{w_{11}^{2}+w_{21}^{2}}} & {\frac{w_{12}^{2}}{w_{12}^{2}+w_{22}^{2}}} \\ {\frac{w_{21}^{2}}{w_{11}^{2}+w_{21}^{2}}} & {\frac{w_{22}^{2}}{w_{12}^{2}+w_{22}^{2}}}\end{array}\right) \cdot \left( \begin{array}{c}{e_{o 1}} \\ {e_{o 2}}\end{array}\right)</script><p>发现只要保持比例不变的话，可以去掉分母，写成：</p>
<script type="math/tex; mode=display">
\left( \begin{array}{c}{e_{h 1}} \\ {e_{h 2}}\end{array}\right)=\left( \begin{array}{cc}{w_{11}^{2}} & {w_{12}^{2}} \\ {w_{21}^{2}} & {w_{22}^{2}}\end{array}\right) \cdot \left( \begin{array}{c}{e_{o 1}} \\ {e_{o 2}}\end{array}\right)</script><p>又发现这个2x2矩阵其实就是前向传播时权值矩阵W的转置，所以又可以改写成：</p>
<script type="math/tex; mode=display">
\mathbf{E}_{h}=\mathbf{W}^{T} \cdot \mathbf{E}_{o}</script><p>于是我们可以利用这个隐藏层的误差来更新权值。</p>
<p>我们现在考虑只有一个输出的情况，如下：<br><img src="https://img-blog.csdnimg.cn/20190423202249357.png" width="50%" alt><br>首先更新$w_{11}^{2}$，我们从输出层开始推导。<br>输出层神经元e的误差：</p>
<script type="math/tex; mode=display">
e_{o 1}=\frac{1}{2}\left(a_{1}^{3}-y_{1}\right)^{2}</script><p>神经元e的输出：</p>
<script type="math/tex; mode=display">
a_{1}^{3}=\operatorname{sigmoid}\left(z_{1}^{3}\right)</script><p>神经元e的输入：</p>
<script type="math/tex; mode=display">
z_{1}^{3}=\left(w_{11}^{2} \cdot a_{1}^{2}+w_{12}^{2} \cdot a_{2}^{2}+b_{1}^{3}\right)</script><p>接下来求e的误差对$w_{11}^{2}$的偏导（链式法则）：</p>
<script type="math/tex; mode=display">
\frac{\partial e_{o 1}}{\partial w_{11}^{2}}=\frac{\partial e_{o 1}}{\partial a_{1}^{3}} \cdot \frac{\partial a_{1}^{3}}{\partial z_{1}^{3}} \cdot \frac{\partial z_{1}^{3}}{\partial w_{11}^{2}}</script><script type="math/tex; mode=display">
\frac{\partial e_{o 1}}{\partial w_{11}^{2}}=\left(a_{1}^{3}-y_{1}\right) \cdot \operatorname{sigmoid}\left(z_{1}^{3}\right) \cdot\left(1-\operatorname{sigmoid}\left(z_{1}^{3}\right)\right) \cdot a_{1}^{2}</script><p>同理也可以求e的误差对$w_{12}^{2}$的偏导：</p>
<script type="math/tex; mode=display">
\frac{\partial e_{o 1}}{\partial w_{12}^{2}}=\frac{\partial e_{o 1}}{\partial a_{1}^{3}} \cdot \frac{\partial a_{1}^{3}}{\partial z_{1}^{3}} \cdot \frac{\partial z_{1}^{3}}{\partial w_{12}^{2}}</script><script type="math/tex; mode=display">
\frac{\partial e_{o 1}}{\overline{\partial} w_{12}^{2}}=\left(a_{1}^{3}-y_{1}\right) \cdot \operatorname{sigmoid}\left(z_{1}^{3}\right) \cdot\left(1-\operatorname{sigmoid}\left(z_{1}^{3}\right)\right) \cdot a_{2}^{2}</script><p>e的误差对偏置的偏导：</p>
<script type="math/tex; mode=display">
\frac{\partial e_{o 1}}{\partial b_{1}^{3}}=\frac{\partial e_{o 1}}{\partial a_{1}^{3}} \cdot \frac{\partial a_{1}^{3}}{\partial z_{1}^{3}} \cdot \frac{\partial z_{1}^{3}}{\partial b_{1}^{3}}</script><script type="math/tex; mode=display">
\frac{\partial e_{o 1}}{\partial b_{1}^{3}}=\left(a_{1}^{3}-y_{1}\right) \cdot \operatorname{sigmoid}\left(z_{1}^{3}\right) \cdot\left(1-\operatorname{sigmoid}\left(z_{1}^{3}\right)\right)</script><p>接下来要对$w_{11}^{1}$进行更新，这次从后向前的推导过程更长一些。</p>
<script type="math/tex; mode=display">
\begin{aligned} e_{o 1} &=\frac{1}{2}\left(a_{1}^{3}-y_{1}\right)^{2} \\ a_{1}^{3} &=\operatorname{sigmoid}\left(z_{1}^{3}\right) \\ z_{1}^{3} &=\left(w_{11}^{2} \cdot a_{1}^{2}+w_{12}^{2} \cdot a_{2}^{2}+b_{1}^{3}\right) \\ a_{1}^{2} &=\operatorname{sigmoid}\left(z_{1}^{2}\right) \\ z_{1}^{2} &=\left(w_{11}^{1} \cdot a_{1}^{1}+w_{12}^{1} \cdot a_{2}^{1}+b_{1}^{2}\right) \end{aligned}</script><p>同样求e的误差对$w_{11}^{1}$的偏导：</p>
<script type="math/tex; mode=display">
\frac{\partial e_{o 1}}{\partial w_{11}^{1}}=\frac{\partial e_{o 1}}{\partial a_{1}^{3}} \cdot \frac{\check{\partial} a_{1}^{3}}{\partial z_{1}^{3}} \cdot \frac{\partial z_{1}^{3}}{\overline{\partial} a_{1}^{2}} \cdot \frac{\partial a_{1}^{2}}{\partial z_{1}^{2}} \cdot \frac{\partial z_{1}^{2}}{\partial w_{11}^{1}}</script><script type="math/tex; mode=display">
\frac{\partial e_{o 1}}{\partial w_{11}^{1}}=\left(a_{1}^{3}-y_{1}\right) \cdot \operatorname{sigmoid}\left(z_{1}^{3}\right) \cdot\left(1-\operatorname{sigmoid}\left(z_{1}^{3}\right)\right) \cdot w_{11}^{2} \cdot \operatorname{sigmoid}\left(z_{1}^{2}\right) \cdot\left(1-\operatorname{sigmoid}\left(z_{1}^{2}\right)\right) \cdot a_{1}^{1}</script><p>$w_{21}^{1}$和偏置同理也可以求得。</p>
<p><strong>求这些偏导是为了做什么？——代入梯度下降来更新权值。</strong><br>比如：</p>
<script type="math/tex; mode=display">
\begin{array}{l}{w=w-\alpha \frac{\partial e_{o 1}}{\partial w}}\end{array}</script><script type="math/tex; mode=display">
\begin{array}{l}{b_{1}=b-\alpha \frac{\partial e_{o 1}}{\partial b}}\end{array}</script><p><strong>问题：那么算法中为什么是$\delta$和$\Delta$？</strong><br>利用链式法则来更新权重方法简单，但过于冗长。由于更新的过程可以看做是从网络的输入层到输出层从前往后更新，每次更新的时候都需要重新计算节点的误差，因此会存在一些不必要的重复计算。所以我们采取先更新后边的权重，之后再在此基础上利用更新后边的权重产生的中间值来更新较靠前的参数的办法。这个中间变量就是下文要介绍的$\delta$变量，它的作用是简化公式并减少计算量。<br>发现在计算偏导数时公式存在相同的部分，如下：</p>
<script type="math/tex; mode=display">
\frac{\partial e_{o 1}}{\partial w_{11}^{2}}={\left(a_{1}^{3}-y_{1}\right) \cdot \operatorname{sigmoid}\left(z_{1}^{3}\right) \cdot\left(1-\operatorname{sigmoid}\left(z_{1}^{3}\right)\right)} \cdot a_{1}^{2}</script><script type="math/tex; mode=display">
\frac{\partial e_{o 1}}{\partial w_{11}^{1}}={\left(a_{1}^{3}-y_{1}\right) \cdot \operatorname{sigmoid}\left(z_{1}^{3}\right) \cdot\left(1-\operatorname{sigmoid}\left(z_{1}^{3}\right)\right)} \cdot w_{11}^{2} \cdot \operatorname{sigmoid}\left(z_{1}^{2}\right) \cdot\left(1-\operatorname{sigmoid}\left(z_{1}^{2}\right)\right) \cdot a_{1}^{1}</script><script type="math/tex; mode=display">
\frac{\partial e_{o 1}}{\partial b_{1}^{3}}={\left(a_{1}^{3}-y_{1}\right) \cdot \operatorname{sigmoid}\left(z_{1}^{3}\right) \cdot\left(1-\operatorname{sigmoid}\left(z_{1}^{3}\right)\right)}</script><p>我们把$\delta_{1}^{3}$定义为这个例子中的输出层神经元e的“误差”：<br>==公式一==</p>
<script type="math/tex; mode=display">
\delta_{1}^{3}=\left(a_{1}^{3}-y_{1}\right) \cdot\left[\operatorname{sigmoid}\left(z_{1}^{3}\right) \cdot\left(1-\operatorname{sigmoid}\left(z_{1}^{3}\right)\right)\right]=\nabla_{a} C \odot \sigma^{\prime}\left(z_{1}^{3}\right)</script><p>隐藏层的误差为：<br>==公式二==</p>
<script type="math/tex; mode=display">
\delta_{1}^{2}={\left(a_{1}^{3}-y_{1}\right) \cdot \operatorname{sigmoid}\left(z_{1}^{3}\right) \cdot\left(1-\operatorname{sigmoid}\left(z_{1}^{3}\right)\right)} \cdot w_{11}^{2} \cdot \operatorname{sigmoid}\left(z_{1}^{2}\right) \cdot\left(1-\operatorname{sigmoid}\left(z_{1}^{2}\right)\right)=\left(\left(w_{11}^{2}\right)^{T} \delta_{1}^{3}\right) \odot \sigma^{\prime}\left(z_{1}^{2}\right)</script><p>权值更新的表示为：<br>==公式三==</p>
<script type="math/tex; mode=display">
\frac{\partial e_{o 1}}{\partial w_{11}^{2}}={\left(a_{1}^{3}-y_{1}\right) \cdot \operatorname{sigmoid}\left(z_{1}^{3}\right) \cdot\left(1-\operatorname{sigmoid}\left(z_{1}^{3}\right)\right)} \cdot a_{1}^{2}=a_{1}^{2} \cdot \delta_{1}^{3}</script><p>偏置的更新表示为：<br>==公式四==</p>
<script type="math/tex; mode=display">
\frac{\partial e_{o 1}}{\partial b_{1}^{3}}={\left(a_{1}^{3}-y_{1}\right) \cdot \operatorname{sigmoid}\left(z_{1}^{3}\right) \cdot\left(1-\operatorname{sigmoid}\left(z_{1}^{3}\right)\right)}=\delta_{1}^{3}</script><p>以上就是<strong>反向传播四大公式</strong>。</p>
<script type="math/tex; mode=display">
\delta^{L}=\frac{\partial e}{\partial a^{L}} \odot \sigma^{\prime}\left(z^{L}\right)</script><script type="math/tex; mode=display">
\delta^{L-1}=\left(W^{L}\right)^{T} \delta^{L} \odot \sigma^{\prime}\left(z^{L-1}\right)</script><script type="math/tex; mode=display">
\frac{\partial e}{b_{j}^{l}}=\delta_{j}^{l}</script><script type="math/tex; mode=display">
\frac{\partial e}{w_{j k}^{l}}=\delta_{j}^{l} a_{k}^{l-1}</script><h1 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h1><ul>
<li>结果可能会收敛到极值。如果只有一个极小值，梯度下降的“爬山”策略一定可以起作用。然而，往往是误差曲面有许多局部最小值和最大值。如果梯度下降的起始点恰好介于局部最大值和局部最小值之间，则沿着梯度下降最大的方向会到达局部最小值。<br><img src="https://img-blog.csdnimg.cn/20190423160717476.png" width="40%" alt></li>
<li>从反向传播学习获得的收敛很慢。</li>
<li>在反向传播学习的收敛性不能保证。</li>
<li>反向传播学习不需要输入向量的标准化（normalization）；然而，标准化可提高性能。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://loyf.github.io/2019/04/16/吴恩达机器学习实验三完整代码/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Loy Fan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Loy With No Title">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/16/吴恩达机器学习实验三完整代码/" itemprop="url">吴恩达机器学习实验三完整代码</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-16T20:54:04+08:00">
                2019-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<p>@<a href="https://loyf.github.io/">LoyFan</a></p>
<blockquote>
<p><a href="https://study.163.com/course/courseMain.htm?courseId=1004570029&amp;_trace_c_p_k2_=ea0c7bf2c97246f08030b6c79f38e69a" target="_blank" rel="noopener">&gt;吴恩达机器学习课程链接</a><br><a href="https://blog.csdn.net/weixin_43318626/article/details/88896788" target="_blank" rel="noopener">&gt;课程总结和笔记链接</a><br>实验三的原始代码和使用数据可至课程链接-课时67-章节9编程作业中下载</p>
<p>包括了实验二中的使用了正则化项后的逻辑回归的最优化参数求解，重点是应用于多分类，采用一对多形式，对每一种分类进行“是/否”预测，得到分类。<br>环境——Matlab R2018b/Octave</p>
</blockquote>
<h1 id="One-vs-all"><a href="#One-vs-all" class="headerlink" title="One-vs-all"></a>One-vs-all</h1><h2 id="Part-1-Loading-and-Visualizing-Data"><a href="#Part-1-Loading-and-Visualizing-Data" class="headerlink" title="Part 1: Loading and Visualizing Data"></a>Part 1: Loading and Visualizing Data</h2><p>这个训练集一共有5000条数据，每条数据包含400维特征。一共10个分类。<br><strong>运行结果</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20190411211940132.png" width="40%" alt></p>
<h2 id="Part-2a-Vectorize-Logistic-Regression"><a href="#Part-2a-Vectorize-Logistic-Regression" class="headerlink" title="Part 2a: Vectorize Logistic Regression"></a>Part 2a: Vectorize Logistic Regression</h2><p><strong>lrCostFunction.m</strong><br>加入正则化项的逻辑回归的损失函数计算，和实验二中的相同。<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">function [J, grad] = lrCostFunction(theta, X, y, lambda)</span><br><span class="line">%LRCOSTFUNCTION Compute cost and gradient <span class="keyword">for</span> logistic regression <span class="keyword">with</span> </span><br><span class="line">%regularization</span><br><span class="line">%   J = LRCOSTFUNCTION(theta, X, y, lambda) computes the cost <span class="keyword">of</span> using</span><br><span class="line">%   theta <span class="keyword">as</span> the parameter <span class="keyword">for</span> regularized logistic regression and the</span><br><span class="line">%   gradient <span class="keyword">of</span> the cost w.r.t. to the parameters. </span><br><span class="line"></span><br><span class="line">% Initialize some useful values</span><br><span class="line">m = length(y); % number <span class="keyword">of</span> training examples</span><br><span class="line"></span><br><span class="line">% You need to <span class="keyword">return</span> the following variables correctly </span><br><span class="line">J = <span class="number">0</span>;</span><br><span class="line">grad = zeros(size(theta));</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the cost <span class="keyword">of</span> a particular choice <span class="keyword">of</span> theta.</span><br><span class="line">%               You should <span class="keyword">set</span> J to the cost.</span><br><span class="line">%               Compute the partial derivatives and <span class="keyword">set</span> grad to the partial</span><br><span class="line">%               derivatives of the cost w.r.t. each parameter in theta</span><br><span class="line">%</span><br><span class="line">% Hint: The computation of the cost function and gradients can be</span><br><span class="line">%       efficiently vectorized. For example, consider the computation</span><br><span class="line">%</span><br><span class="line">%           sigmoid(X * theta)</span><br><span class="line">%</span><br><span class="line">%       Each row of the resulting matrix will contain the value of the</span><br><span class="line">%       prediction for that example. You can make use of this to vectorize</span><br><span class="line">%       the cost function and gradient computations. </span><br><span class="line">%</span><br><span class="line">% Hint: When computing the gradient of the regularized cost function, </span><br><span class="line">%       there're many possible vectorized solutions, but one solution</span><br><span class="line">%       looks like:</span><br><span class="line">%           grad = (unregularized gradient for logistic regression)</span><br><span class="line">%           temp = theta; </span><br><span class="line">%           temp(1) = 0;   % because we don't add anything for j = 0  </span><br><span class="line">%           grad = grad + YOUR_CODE_HERE (using the temp variable)</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">pos = y == 1;</span><br><span class="line">neg = y == 0;</span><br><span class="line"></span><br><span class="line">h_pos = sigmoid(X(pos, :) * theta);</span><br><span class="line">J_pos = sum(-log(h_pos));</span><br><span class="line"></span><br><span class="line">h_neg = sigmoid(X(neg, :) * theta);</span><br><span class="line">J_neg = sum(-log(1 - h_neg));</span><br><span class="line"></span><br><span class="line">J_reg = lambda/2 * sum(theta(2:end, :) .^ 2);</span><br><span class="line">J = (J_pos + J_neg + J_reg)/m;</span><br><span class="line"></span><br><span class="line">grad = (sum(X .* (sigmoid(X * theta) - y)))' / m;</span><br><span class="line">grad_reg = ((lambda * theta(2:end, :)) / m);</span><br><span class="line">grad(2:end, :) = grad(2:end, :) + grad_reg;</span><br><span class="line"></span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line">grad = grad(:);</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure></p>
<p><strong>运行结果</strong></p>
<pre><code>Testing lrCostFunction() with regularization
Cost: 2.534819
Expected cost: 2.534819
Gradients:
 0.146561 
 -0.548558 
 0.724722 
 1.398003 
Expected gradients:
 0.146561
 -0.548558
 0.724722
 1.398003
Program paused. Press enter to continue.
</code></pre><h2 id="Part-2b-One-vs-All-Training"><a href="#Part-2b-One-vs-All-Training" class="headerlink" title="Part 2b: One-vs-All Training"></a>Part 2b: One-vs-All Training</h2><p><strong>oneVsAll.m</strong><br>通过最小化损失函数计算所有十个类别的最优参数。<br>参数为10行400列矩阵。<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"> function [all_theta] = oneVsAll(X, y, num_labels, lambda)</span><br><span class="line">%ONEVSALL trains multiple logistic regression classifiers and returns all</span><br><span class="line">%the classifiers <span class="keyword">in</span> a matrix all_theta, where the i-th row <span class="keyword">of</span> all_theta </span><br><span class="line">%corresponds to the classifier <span class="keyword">for</span> label i</span><br><span class="line">%   [all_theta] = ONEVSALL(X, y, num_labels, lambda) trains num_labels</span><br><span class="line">%   logistic regression classifiers and returns each <span class="keyword">of</span> these classifiers</span><br><span class="line">%   <span class="keyword">in</span> a matrix all_theta, where the i-th row <span class="keyword">of</span> all_theta corresponds </span><br><span class="line">%   to the classifier <span class="keyword">for</span> label i</span><br><span class="line"></span><br><span class="line">% Some useful variables</span><br><span class="line">m = size(X, <span class="number">1</span>);%<span class="number">5000</span></span><br><span class="line">n = size(X, <span class="number">2</span>);%<span class="number">400</span></span><br><span class="line"></span><br><span class="line">% You need to <span class="keyword">return</span> the following variables correctly </span><br><span class="line">all_theta = zeros(num_labels, n + <span class="number">1</span>);%<span class="number">10</span>*<span class="number">401</span></span><br><span class="line"></span><br><span class="line">% Add ones to the X data matrix</span><br><span class="line">X = [ones(m, <span class="number">1</span>) X];</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: You should complete the following code to train num_labels</span><br><span class="line">%               logistic regression classifiers <span class="keyword">with</span> regularization</span><br><span class="line">%               parameter lambda. </span><br><span class="line">%</span><br><span class="line">% Hint: theta(:) will <span class="keyword">return</span> a column vector.</span><br><span class="line">%</span><br><span class="line">% Hint: You can use y == c to obtain a vector <span class="keyword">of</span> <span class="number">1</span><span class="string">'s and 0'</span>s that tell you</span><br><span class="line">%       whether the ground truth is <span class="literal">true</span>/<span class="literal">false</span> <span class="keyword">for</span> <span class="keyword">this</span> <span class="class"><span class="keyword">class</span>.</span></span><br><span class="line"><span class="class">%</span></span><br><span class="line">% Note: For this assignment, we recommend using fmincg to optimize the cost</span><br><span class="line">%       <span class="function"><span class="keyword">function</span>. <span class="title">It</span> <span class="title">is</span> <span class="title">okay</span> <span class="title">to</span> <span class="title">use</span> <span class="title">a</span> <span class="title">for</span>-<span class="title">loop</span> (<span class="params">for c = <span class="number">1</span>:num_labels</span>) <span class="title">to</span></span></span><br><span class="line">%       loop over the different classes.</span><br><span class="line">%</span><br><span class="line">%       fmincg works similarly to fminunc, but is more efficient when we</span><br><span class="line">%       are dealing <span class="keyword">with</span> large number <span class="keyword">of</span> parameters.</span><br><span class="line">%</span><br><span class="line">% Example Code <span class="keyword">for</span> fmincg:</span><br><span class="line">%</span><br><span class="line">%     % <span class="built_in">Set</span> Initial theta</span><br><span class="line">%     initial_theta = zeros(n + <span class="number">1</span>, <span class="number">1</span>);</span><br><span class="line">%     </span><br><span class="line">%     % <span class="built_in">Set</span> options <span class="keyword">for</span> fminunc</span><br><span class="line">%     options = optimset(<span class="string">'GradObj'</span>, <span class="string">'on'</span>, <span class="string">'MaxIter'</span>, <span class="number">50</span>);</span><br><span class="line">% </span><br><span class="line">%     % Run fmincg to obtain the optimal theta</span><br><span class="line">%     % This <span class="function"><span class="keyword">function</span> <span class="title">will</span> <span class="title">return</span> <span class="title">theta</span> <span class="title">and</span> <span class="title">the</span> <span class="title">cost</span> </span></span><br><span class="line">%     [theta] = ...</span><br><span class="line">%         fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), ...</span><br><span class="line">%                 initial_theta, options);</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> c = <span class="number">1</span>:num_labels</span><br><span class="line"></span><br><span class="line">    % Initialize fitting parameters</span><br><span class="line">    initial_theta = zeros(n + <span class="number">1</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    % <span class="built_in">Set</span> Options</span><br><span class="line">    options = optimset(<span class="string">'GradObj'</span>, <span class="string">'on'</span>, <span class="string">'MaxIter'</span>, <span class="number">50</span>);</span><br><span class="line"></span><br><span class="line">    % Optimize</span><br><span class="line">    [all_theta(c, :), J, exit_flag] = ...</span><br><span class="line">        fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), initial_theta, options);</span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure></p>
<h2 id="Part-3-Predict-for-One-Vs-All"><a href="#Part-3-Predict-for-One-Vs-All" class="headerlink" title="Part 3: Predict for One-Vs-All"></a>Part 3: Predict for One-Vs-All</h2><p><strong>predictOneVsAll.m</strong><br>使用得到的10<em>400参数对数据进行预测（用训练集预测哈哈哈）<br>首先得到all_p为5000</em>10的矩阵，包含0、1，得到5000个数据的预测分类<br>找到每一行的1值序号，得到p为5000*1的向量，是每一个数据的预测分类。<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">p</span> = <span class="title">predictOneVsAll</span>(<span class="params">all_theta, X</span>)</span></span><br><span class="line">%PREDICT Predict the label for a trained one-vs-all classifier. The labels </span><br><span class="line">%are <span class="keyword">in</span> the range <span class="number">1.</span>.K, where K = size(all_theta, <span class="number">1</span>). </span><br><span class="line">%  p = PREDICTONEVSALL(all_theta, X) will <span class="keyword">return</span> a vector <span class="keyword">of</span> predictions</span><br><span class="line">%  <span class="keyword">for</span> each example <span class="keyword">in</span> the matrix X. Note that X contains the examples <span class="keyword">in</span></span><br><span class="line">%  rows. all_theta is a matrix where the i-th row is a trained logistic</span><br><span class="line">%  regression theta vector <span class="keyword">for</span> the i-th <span class="class"><span class="keyword">class</span>. <span class="title">You</span> <span class="title">should</span> <span class="title">set</span> <span class="title">p</span> <span class="title">to</span> <span class="title">a</span> <span class="title">vector</span></span></span><br><span class="line"><span class="class">%  <span class="title">of</span> <span class="title">values</span> <span class="title">from</span> 1..<span class="title">K</span> (<span class="title">e</span>.<span class="title">g</span>., <span class="title">p</span> </span>= [<span class="number">1</span>; <span class="number">3</span>; <span class="number">1</span>; <span class="number">2</span>] predicts classes <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">%  <span class="keyword">for</span> <span class="number">4</span> examples) </span><br><span class="line"></span><br><span class="line">m = size(X, <span class="number">1</span>);%<span class="number">5000</span></span><br><span class="line">num_labels = size(all_theta, <span class="number">1</span>);%<span class="number">10</span></span><br><span class="line"></span><br><span class="line">% You need to <span class="keyword">return</span> the following variables correctly </span><br><span class="line">p = zeros(size(X, <span class="number">1</span>), <span class="number">1</span>);%<span class="number">5000</span>*<span class="number">1</span></span><br><span class="line"></span><br><span class="line">% Add ones to the X data matrix</span><br><span class="line">X = [ones(m, <span class="number">1</span>) X];</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Complete the following code to make predictions using</span><br><span class="line">%               your learned logistic regression parameters (one-vs-all).</span><br><span class="line">%               You should <span class="keyword">set</span> p to a vector of predictions (from 1 to</span><br><span class="line">%               num_labels).</span><br><span class="line">%</span><br><span class="line">% Hint: This code can be done all vectorized using the max function.</span><br><span class="line">%       In particular, the max function can also return the index of the </span><br><span class="line">%       max element, for more information see 'help max'. If your examples </span><br><span class="line">%       are in rows, then, you can use max(A, [], 2) to obtain the max </span><br><span class="line">%       for each row.</span><br><span class="line">%       </span><br><span class="line"></span><br><span class="line">all_p = floor(sigmoid(X * (all_theta)') / 0.5);</span><br><span class="line">[val, p] = max(all_p, [], 2);</span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure></p>
<p><strong>运行结果</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Training <span class="built_in">Set</span> Accuracy: <span class="number">89.160000</span></span><br></pre></td></tr></table></figure></p>
<h2 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h2><p><strong>ex3.m</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class - Exercise <span class="number">3</span> | Part <span class="number">1</span>: One-vs-all</span><br><span class="line"></span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">%</span><br><span class="line">%  This file contains code that helps you <span class="keyword">get</span> started on the</span><br><span class="line">%  linear exercise. You will need to complete the following functions</span><br><span class="line">%  in this exericse:</span><br><span class="line">%</span><br><span class="line">%     lrCostFunction.m (logistic regression cost function)</span><br><span class="line">%     oneVsAll.m</span><br><span class="line">%     predictOneVsAll.m</span><br><span class="line">%     predict.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">%% Setup the parameters you will use for this part of the exercise</span><br><span class="line">input_layer_size  = 400;  % 20x20 Input Images of Digits</span><br><span class="line">num_labels = 10;          % 10 labels, from 1 to 10</span><br><span class="line">                          % (note that we have mapped "0" to label 10)</span><br><span class="line"></span><br><span class="line">%% =========== Part 1: Loading and Visualizing Data =============</span><br><span class="line">%  We start the exercise by first loading and visualizing the dataset.</span><br><span class="line">%  You will be working with a dataset that contains handwritten digits.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Load Training Data</span><br><span class="line">fprintf('Loading and Visualizing Data ...\n')</span><br><span class="line"></span><br><span class="line">load('ex3data1.mat'); % training data stored in arrays X, y</span><br><span class="line">m = size(X, 1);</span><br><span class="line"></span><br><span class="line">% Randomly select 100 data points to display</span><br><span class="line">rand_indices = randperm(m);</span><br><span class="line">sel = X(rand_indices(1:100), :);</span><br><span class="line"></span><br><span class="line">displayData(sel);</span><br><span class="line"></span><br><span class="line">fprintf('Program paused. Press enter to continue.\n');</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ============ Part 2a: Vectorize Logistic Regression ============</span><br><span class="line">%  In this part of the exercise, you will reuse your logistic regression</span><br><span class="line">%  code from the last exercise. You task here is to make sure that your</span><br><span class="line">%  regularized logistic regression implementation is vectorized. After</span><br><span class="line">%  that, you will implement one-vs-all classification for the handwritten</span><br><span class="line">%  digit dataset.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Test case for lrCostFunction</span><br><span class="line">fprintf('\nTesting lrCostFunction() with regularization');</span><br><span class="line"></span><br><span class="line">theta_t = [-2; -1; 1; 2];</span><br><span class="line">X_t = [ones(5,1) reshape(1:15,5,3)/10];</span><br><span class="line">y_t = ([1;0;1;0;1] &gt;= 0.5);</span><br><span class="line">lambda_t = 3;</span><br><span class="line">[J grad] = lrCostFunction(theta_t, X_t, y_t, lambda_t);</span><br><span class="line"></span><br><span class="line">fprintf('\nCost: %f\n', J);</span><br><span class="line">fprintf('Expected cost: 2.534819\n');</span><br><span class="line">fprintf('Gradients:\n');</span><br><span class="line">fprintf(' %f \n', grad);</span><br><span class="line">fprintf('Expected gradients:\n');</span><br><span class="line">fprintf(' 0.146561\n -0.548558\n 0.724722\n 1.398003\n');</span><br><span class="line"></span><br><span class="line">fprintf('Program paused. Press enter to continue.\n');</span><br><span class="line">pause;</span><br><span class="line">%% ============ Part 2b: One-vs-All Training ============</span><br><span class="line">fprintf('\nTraining One-vs-All Logistic Regression...\n')</span><br><span class="line"></span><br><span class="line">lambda = 0.1;</span><br><span class="line">[all_theta] = oneVsAll(X, y, num_labels, lambda);</span><br><span class="line"></span><br><span class="line">fprintf('Program paused. Press enter to continue.\n');</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ================ Part 3: Predict for One-Vs-All ================</span><br><span class="line"></span><br><span class="line">pred = predictOneVsAll(all_theta, X);</span><br><span class="line"></span><br><span class="line">fprintf('\nTraining Set Accuracy: %f\n', mean(double(pred == y)) * 100);</span><br></pre></td></tr></table></figure></p>
<h1 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h1><h2 id="Part-1-Loading-and-Visualizing-Data-1"><a href="#Part-1-Loading-and-Visualizing-Data-1" class="headerlink" title="Part 1: Loading and Visualizing Data"></a>Part 1: Loading and Visualizing Data</h2><p><img src="https://img-blog.csdnimg.cn/20190414143413118.png" width="40%" alt></p>
<h2 id="Part-2-Loading-Pameters"><a href="#Part-2-Loading-Pameters" class="headerlink" title="Part 2: Loading Pameters"></a>Part 2: Loading Pameters</h2><h2 id="Part-3-Implement-Predict"><a href="#Part-3-Implement-Predict" class="headerlink" title="Part 3: Implement Predict"></a>Part 3: Implement Predict</h2><p><strong>predict.m</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">p</span> = <span class="title">predict</span>(<span class="params">Theta1, Theta2, X</span>)</span></span><br><span class="line">%PREDICT Predict the label of an input given a trained neural network</span><br><span class="line">%   p = PREDICT(Theta1, Theta2, X) outputs the predicted label <span class="keyword">of</span> X given the</span><br><span class="line">%   trained weights <span class="keyword">of</span> a neural network (Theta1, Theta2)</span><br><span class="line"></span><br><span class="line">% Useful values</span><br><span class="line">m = size(X, <span class="number">1</span>);</span><br><span class="line">num_labels = size(Theta2, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">% You need to <span class="keyword">return</span> the following variables correctly </span><br><span class="line">p = zeros(size(X, <span class="number">1</span>), <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Complete the following code to make predictions using</span><br><span class="line">%               your learned neural network. You should <span class="keyword">set</span> p to a </span><br><span class="line">%               vector containing labels between 1 to num_labels.</span><br><span class="line">%</span><br><span class="line">% Hint: The max function might come in useful. In particular, the max</span><br><span class="line">%       function can also return the index of the max element, for more</span><br><span class="line">%       information see 'help max'. If your examples are in rows, then, you</span><br><span class="line">%       can use max(A, [], 2) to obtain the max for each row.</span><br><span class="line">%</span><br><span class="line">X = [ones(m, 1) X];</span><br><span class="line">all_p = floor(sigmoid(X * (Theta1)') / 0.5);</span><br><span class="line">all_p = [ones(m, 1) all_p];</span><br><span class="line">all_p = floor(sigmoid(all_p * (Theta2)') / 0.5);</span><br><span class="line">[val, p] = max(all_p, [], 2);</span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure></p>
<p><strong>实验结果</strong></p>
<pre><code>Loading and Visualizing Data ...
Program paused. Press enter to continue.

Loading Saved Neural Network Parameters ...

Training Set Accuracy: 95.400000
Program paused. Press enter to continue.

Displaying Example Image

Neural Network Prediction: 8 (digit 8)
Paused - press enter to continue, q to exit:

Displaying Example Image

Neural Network Prediction: 2 (digit 2)
Paused - press enter to continue, q to exit:
</code></pre><p><img src="https://img-blog.csdnimg.cn/20190414143447894.png" width="40%" alt><br><img src="https://img-blog.csdnimg.cn/20190414143509662.png" width="40%" alt></p>
<h2 id="主函数-1"><a href="#主函数-1" class="headerlink" title="主函数"></a>主函数</h2><p><strong>ex3_nn.m</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class - Exercise <span class="number">3</span> | Part <span class="number">2</span>: Neural Networks</span><br><span class="line"></span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">% </span><br><span class="line">%  This file contains code that helps you <span class="keyword">get</span> started on the</span><br><span class="line">%  linear exercise. You will need to complete the following functions </span><br><span class="line">%  in this exericse:</span><br><span class="line">%</span><br><span class="line">%     lrCostFunction.m (logistic regression cost function)</span><br><span class="line">%     oneVsAll.m</span><br><span class="line">%     predictOneVsAll.m</span><br><span class="line">%     predict.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">%% Setup the parameters you will use for this exercise</span><br><span class="line">input_layer_size  = 400;  % 20x20 Input Images of Digits</span><br><span class="line">hidden_layer_size = 25;   % 25 hidden units</span><br><span class="line">num_labels = 10;          % 10 labels, from 1 to 10   </span><br><span class="line">                          % (note that we have mapped "0" to label 10)</span><br><span class="line"></span><br><span class="line">%% =========== Part 1: Loading and Visualizing Data =============</span><br><span class="line">%  We start the exercise by first loading and visualizing the dataset. </span><br><span class="line">%  You will be working with a dataset that contains handwritten digits.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Load Training Data</span><br><span class="line">fprintf('Loading and Visualizing Data ...\n')</span><br><span class="line"></span><br><span class="line">load('ex3data1.mat');</span><br><span class="line">m = size(X, 1);%5000</span><br><span class="line"></span><br><span class="line">% Randomly select 100 data points to display</span><br><span class="line">sel = randperm(size(X, 1));</span><br><span class="line">sel = sel(1:100);</span><br><span class="line"></span><br><span class="line">displayData(X(sel, :));</span><br><span class="line"></span><br><span class="line">fprintf('Program paused. Press enter to continue.\n');</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ================ Part 2: Loading Pameters ================</span><br><span class="line">% In this part of the exercise, we load some pre-initialized </span><br><span class="line">% neural network parameters.</span><br><span class="line"></span><br><span class="line">fprintf('\nLoading Saved Neural Network Parameters ...\n')</span><br><span class="line"></span><br><span class="line">% Load the weights into variables Theta1 and Theta2</span><br><span class="line">load('ex3weights.mat');%Theta1 25*401,Theta2 10*26</span><br><span class="line"></span><br><span class="line">%% ================= Part 3: Implement Predict =================</span><br><span class="line">%  After training the neural network, we would like to use it to predict</span><br><span class="line">%  the labels. You will now implement the "predict" function to use the</span><br><span class="line">%  neural network to predict the labels of the training <span class="keyword">set</span>. This lets</span><br><span class="line">%  you compute the training <span class="keyword">set</span> accuracy.</span><br><span class="line"></span><br><span class="line">pred = predict(Theta1, Theta2, X);</span><br><span class="line"></span><br><span class="line">fprintf('\nTraining Set Accuracy: %f\n', mean(double(pred == y)) * 100);</span><br><span class="line"></span><br><span class="line">fprintf('Program paused. Press enter to continue.\n');</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%  To give you an idea of the network's output, you can also run</span><br><span class="line">%  through the examples one at the a time to see what it is predicting.</span><br><span class="line"></span><br><span class="line">%  Randomly permute examples</span><br><span class="line">rp = randperm(m);</span><br><span class="line"></span><br><span class="line">for i = 1:m</span><br><span class="line">    % Display </span><br><span class="line">    fprintf('\nDisplaying Example Image\n');</span><br><span class="line">    displayData(X(rp(i), :));</span><br><span class="line"></span><br><span class="line">    pred = predict(Theta1, Theta2, X(rp(i),:));</span><br><span class="line">    fprintf('\nNeural Network Prediction: %d (digit %d)\n', pred, mod(pred, 10));</span><br><span class="line">    </span><br><span class="line">    % Pause with quit option</span><br><span class="line">    s = input('Paused - press enter to continue, q to exit:','s');</span><br><span class="line">    if s == 'q'</span><br><span class="line">      break</span><br><span class="line">    end</span><br><span class="line">end</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>实验三完成</p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://loyf.github.io/2019/04/16/吴恩达机器学习实验二完整代码/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Loy Fan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Loy With No Title">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/16/吴恩达机器学习实验二完整代码/" itemprop="url">吴恩达机器学习实验二完整代码</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-16T20:54:00+08:00">
                2019-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<p>@<a href="https://loyf.github.io/">LoyFan</a></p>
<blockquote>
<p><a href="https://study.163.com/course/courseMain.htm?courseId=1004570029&amp;_trace_c_p_k2_=ea0c7bf2c97246f08030b6c79f38e69a" target="_blank" rel="noopener">&gt;吴恩达机器学习课程链接</a><br><a href="https://blog.csdn.net/weixin_43318626/article/details/88896788" target="_blank" rel="noopener">&gt;课程总结和笔记链接</a><br>实验二的原始代码和使用数据可至课程链接-课时60-章节8编程作业中下载</p>
<p>包括逻辑回归的损失函数、梯度、自动优化、预测以及正则化后的损失函数、梯度等<br>环境——Matlab R2018b/Octave</p>
</blockquote>
<h1 id="一般Logistic-Regression"><a href="#一般Logistic-Regression" class="headerlink" title="一般Logistic Regression"></a>一般Logistic Regression</h1><h2 id="Part-1-Plotting"><a href="#Part-1-Plotting" class="headerlink" title="Part 1: Plotting"></a>Part 1: Plotting</h2><p><strong>plotData.m</strong><br>二分类，在图上用不同的标记表示两类数据<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">plotData</span>(<span class="params">X, y</span>)</span></span><br><span class="line">%PLOTDATA Plots the data points X and y into a new figure </span><br><span class="line">%   PLOTDATA(x,y) plots the data points <span class="keyword">with</span> + <span class="keyword">for</span> the positive examples</span><br><span class="line">%   and o <span class="keyword">for</span> the negative examples. X is assumed to be a Mx2 matrix.</span><br><span class="line"></span><br><span class="line">% Create New Figure</span><br><span class="line">figure; hold on;</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Plot the positive and negative examples on a</span><br><span class="line">%               <span class="number">2</span>D plot, using the option <span class="string">'k+'</span> <span class="keyword">for</span> the positive</span><br><span class="line">%               examples and <span class="string">'ko'</span> <span class="keyword">for</span> the negative examples.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">positive = find(y == <span class="number">1</span>);</span><br><span class="line">negative = find(y == <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">plot(X(positive, <span class="number">1</span>), X(positive, <span class="number">2</span>), <span class="string">'k+'</span>)</span><br><span class="line">plot(X(negative, <span class="number">1</span>), X(negative, <span class="number">2</span>), <span class="string">'ko'</span>, <span class="string">'MarkerFaceColor'</span>, <span class="string">'b'</span>)</span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line"></span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure></p>
<p><strong>运行结果</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20190405115002961.png" width="60%" alt></p>
<h2 id="Part-2-Compute-Cost-and-Gradient"><a href="#Part-2-Compute-Cost-and-Gradient" class="headerlink" title="Part 2: Compute Cost and Gradient"></a>Part 2: Compute Cost and Gradient</h2><p><strong>sigmoid.m</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">g</span> = <span class="title">sigmoid</span>(<span class="params">z</span>)</span></span><br><span class="line">%SIGMOID Compute sigmoid function</span><br><span class="line">%   g = SIGMOID(z) computes the sigmoid <span class="keyword">of</span> z.</span><br><span class="line"></span><br><span class="line">% You need to <span class="keyword">return</span> the following variables correctly </span><br><span class="line">g = zeros(size(z));</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the sigmoid <span class="keyword">of</span> each value <span class="keyword">of</span> z (z can be a matrix,</span><br><span class="line">%               vector or scalar).</span><br><span class="line"></span><br><span class="line">g = <span class="number">1</span> ./ (exp(-z)+<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure></p>
<p><strong>costFunction.m</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">function [J, grad] = costFunction(theta, X, y)</span><br><span class="line">%COSTFUNCTION Compute cost and gradient <span class="keyword">for</span> logistic regression</span><br><span class="line">%   J = COSTFUNCTION(theta, X, y) computes the cost <span class="keyword">of</span> using theta <span class="keyword">as</span> the</span><br><span class="line">%   parameter <span class="keyword">for</span> logistic regression and the gradient <span class="keyword">of</span> the cost</span><br><span class="line">%   w.r.t. to the parameters.</span><br><span class="line"></span><br><span class="line">% Initialize some useful values</span><br><span class="line">m = length(y); % number <span class="keyword">of</span> training examples</span><br><span class="line"></span><br><span class="line">% You need to <span class="keyword">return</span> the following variables correctly </span><br><span class="line">J = <span class="number">0</span>;</span><br><span class="line">grad = zeros(size(theta));</span><br><span class="line">alpha = <span class="number">0.01</span>;</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the cost <span class="keyword">of</span> a particular choice <span class="keyword">of</span> theta.</span><br><span class="line">%               You should <span class="keyword">set</span> J to the cost.</span><br><span class="line">%               Compute the partial derivatives and <span class="keyword">set</span> grad to the partial</span><br><span class="line">%               derivatives of the cost w.r.t. each parameter in theta</span><br><span class="line">%</span><br><span class="line">% Note: grad should have the same dimensions as theta</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">pos = y == 1;</span><br><span class="line">neg = y == 0;</span><br><span class="line"></span><br><span class="line">h_pos = sigmoid(X(pos, :) * theta);</span><br><span class="line">J_pos = sum(-log(h_pos));</span><br><span class="line"></span><br><span class="line">h_neg = sigmoid(X(neg, :) * theta);</span><br><span class="line">J_neg = sum(-log(1 - h_neg));</span><br><span class="line"></span><br><span class="line">J = (J_pos + J_neg)/m;</span><br><span class="line"></span><br><span class="line">grad = (sum(X .* (sigmoid(X * theta) - y)))' * alpha;</span><br><span class="line"></span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure></p>
<p><strong>运行结果</strong></p>
<pre><code>Cost at initial theta (zeros): 0.693147
Expected cost (approx): 0.693
Gradient at initial theta (zeros): 
 -0.100000 
 -12.009217 
 -11.262842 
Expected gradients (approx):
 -0.1000
 -12.0092
 -11.2628

Cost at test theta: 0.218330
Expected cost (approx): 0.218
Gradient at test theta: 
 0.042903 
 2.566234 
 2.646797 
Expected gradients (approx):
 0.043
 2.566
 2.647

Program paused. Press enter to continue.
</code></pre><h2 id="Part-3-Optimizing-using-fminunc"><a href="#Part-3-Optimizing-using-fminunc" class="headerlink" title="Part 3: Optimizing using fminunc"></a>Part 3: Optimizing using fminunc</h2><p><strong>plotDecisionBoundary.m</strong><br>画出决策边界<br><img src="https://img-blog.csdnimg.cn/20190406133741605.png" width="60%" alt><br>使用自动寻找最优参数函数（代码在主函数中）<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[theta, cost] = ...</span><br><span class="line">	fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);</span><br></pre></td></tr></table></figure></p>
<p><strong>运行结果</strong></p>
<pre><code>Cost at theta found by fminunc: 0.203498
Expected cost (approx): 0.203
theta: 
 -25.161343 
 0.206232 
 0.201472 
Expected theta (approx):
 -25.161
 0.206
 0.201
</code></pre><h2 id="Part-4-Predict-and-Accuracies"><a href="#Part-4-Predict-and-Accuracies" class="headerlink" title="Part 4: Predict and Accuracies"></a>Part 4: Predict and Accuracies</h2><p>预测一个实例&amp;&amp;查看模型在训练集上的准确率<br><strong>predict.m</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">p</span> = <span class="title">predict</span>(<span class="params">theta, X</span>)</span></span><br><span class="line">%PREDICT Predict whether the label is 0 or 1 using learned logistic </span><br><span class="line">%regression parameters theta</span><br><span class="line">%   p = PREDICT(theta, X) computes the predictions <span class="keyword">for</span> X using a </span><br><span class="line">%   threshold at <span class="number">0.5</span> (i.e., <span class="keyword">if</span> sigmoid(theta<span class="string">'*x) &gt;= 0.5, predict 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">m = size(X, 1); % Number of training examples</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">% You need to return the following variables correctly</span></span><br><span class="line"><span class="string">p = zeros(m, 1);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">% ====================== YOUR CODE HERE ======================</span></span><br><span class="line"><span class="string">% Instructions: Complete the following code to make predictions using</span></span><br><span class="line"><span class="string">%               your learned logistic regression parameters. </span></span><br><span class="line"><span class="string">%               You should set p to a vector of 0'</span>s and <span class="number">1</span><span class="string">'s</span></span><br><span class="line"><span class="string">%</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">p = floor(sigmoid(X * theta) / 0.5);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">% =========================================================================</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">end</span></span><br></pre></td></tr></table></figure></p>
<p><strong>运行结果</strong></p>
<pre><code>For a student with scores 45 and 85, we predict an admission probability of 0.776291
Expected value: 0.775 +/- 0.002

Train Accuracy: 89.000000
Expected accuracy (approx): 89.0
</code></pre><h2 id="主函数代码"><a href="#主函数代码" class="headerlink" title="主函数代码"></a>主函数代码</h2><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class - Exercise <span class="number">2</span>: Logistic Regression</span><br><span class="line">%</span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">% </span><br><span class="line">%  This file contains code that helps you <span class="keyword">get</span> started on the logistic</span><br><span class="line">%  regression exercise. You will need to complete the following functions </span><br><span class="line">%  in this exericse:</span><br><span class="line">%5</span><br><span class="line">%     plotData.m</span><br><span class="line">%     sigmoid.m</span><br><span class="line">%     costFunction.m</span><br><span class="line">%     predict.m</span><br><span class="line">%     costFunctionReg.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">%% Load Data</span><br><span class="line">%  The first two columns contains the exam scores and the third column</span><br><span class="line">%  contains the label.</span><br><span class="line"></span><br><span class="line">data = load('ex2data1.txt');</span><br><span class="line">X = data(:, [1, 2]); y = data(:, 3);</span><br><span class="line"></span><br><span class="line">%% ==================== Part 1: Plotting ====================</span><br><span class="line">%  We start the exercise by first plotting the data to understand the </span><br><span class="line">%  the problem we are working with.</span><br><span class="line"></span><br><span class="line">fprintf(['Plotting data with + indicating (y = 1) examples and o ' ...</span><br><span class="line">         'indicating (y = 0) examples.\n']);</span><br><span class="line"></span><br><span class="line">plotData(X, y);</span><br><span class="line"></span><br><span class="line">% Put some labels </span><br><span class="line">hold on;</span><br><span class="line">% Labels and Legend</span><br><span class="line">xlabel('Exam 1 score')</span><br><span class="line">ylabel('Exam 2 score')</span><br><span class="line"></span><br><span class="line">% Specified in plot order</span><br><span class="line">legend('Admitted', 'Not admitted')</span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line">fprintf('\nProgram paused. Press enter to continue.\n');</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ============ Part 2: Compute Cost and Gradient ============</span><br><span class="line">%  In this part of the exercise, you will implement the cost and gradient</span><br><span class="line">%  for logistic regression. You neeed to complete the code in </span><br><span class="line">%  costFunction.m</span><br><span class="line"></span><br><span class="line">%  Setup the data matrix appropriately, and add ones for the intercept term</span><br><span class="line">[m, n] = size(X);</span><br><span class="line"></span><br><span class="line">% Add intercept term to x and X_test</span><br><span class="line">X = [ones(m, 1) X];</span><br><span class="line"></span><br><span class="line">% Initialize fitting parameters</span><br><span class="line">initial_theta = zeros(n + 1, 1);</span><br><span class="line"></span><br><span class="line">% Compute and display initial cost and gradient</span><br><span class="line">[cost, grad] = costFunction(initial_theta, X, y);</span><br><span class="line"></span><br><span class="line">fprintf('Cost at initial theta (zeros): %f\n', cost);</span><br><span class="line">fprintf('Expected cost (approx): 0.693\n');</span><br><span class="line">fprintf('Gradient at initial theta (zeros): \n');</span><br><span class="line">fprintf(' %f \n', grad);</span><br><span class="line">fprintf('Expected gradients (approx):\n -0.1000\n -12.0092\n -11.2628\n');</span><br><span class="line"></span><br><span class="line">% Compute and display cost and gradient with non-zero theta</span><br><span class="line">test_theta = [-24; 0.2; 0.2];</span><br><span class="line">[cost, grad] = costFunction(test_theta, X, y);</span><br><span class="line"></span><br><span class="line">fprintf('\nCost at test theta: %f\n', cost);</span><br><span class="line">fprintf('Expected cost (approx): 0.218\n');</span><br><span class="line">fprintf('Gradient at test theta: \n');</span><br><span class="line">fprintf(' %f \n', grad);</span><br><span class="line">fprintf('Expected gradients (approx):\n 0.043\n 2.566\n 2.647\n');</span><br><span class="line"></span><br><span class="line">fprintf('\nProgram paused. Press enter to continue.\n');</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ============= Part 3: Optimizing using fminunc  =============</span><br><span class="line">%  In this exercise, you will use a built-in function (fminunc) to find the</span><br><span class="line">%  optimal parameters theta.</span><br><span class="line"></span><br><span class="line">%  Set options for fminunc</span><br><span class="line">options = optimset('GradObj', 'on', 'MaxIter', 400);</span><br><span class="line"></span><br><span class="line">%  Run fminunc to obtain the optimal theta</span><br><span class="line">%  This function will return theta and the cost </span><br><span class="line">[theta, cost] = ...</span><br><span class="line">	fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);</span><br><span class="line"></span><br><span class="line">% Print theta to screen</span><br><span class="line">fprintf('Cost at theta found by fminunc: %f\n', cost);</span><br><span class="line">fprintf('Expected cost (approx): 0.203\n');</span><br><span class="line">fprintf('theta: \n');</span><br><span class="line">fprintf(' %f \n', theta);</span><br><span class="line">fprintf('Expected theta (approx):\n');</span><br><span class="line">fprintf(' -25.161\n 0.206\n 0.201\n');</span><br><span class="line"></span><br><span class="line">% Plot Boundary</span><br><span class="line">plotDecisionBoundary(theta, X, y);</span><br><span class="line"></span><br><span class="line">% Put some labels </span><br><span class="line">hold on;</span><br><span class="line">% Labels and Legend</span><br><span class="line">xlabel('Exam 1 score')</span><br><span class="line">ylabel('Exam 2 score')</span><br><span class="line"></span><br><span class="line">% Specified in plot order</span><br><span class="line">legend('Admitted', 'Not admitted')</span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line">fprintf('\nProgram paused. Press enter to continue.\n');</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ============== Part 4: Predict and Accuracies ==============</span><br><span class="line">%  After learning the parameters, you'll like to use it to predict the outcomes</span><br><span class="line">%  on unseen data. In this part, you will use the logistic regression model</span><br><span class="line">%  to predict the probability that a student with score 45 on exam 1 and </span><br><span class="line">%  score 85 on exam 2 will be admitted.</span><br><span class="line">%</span><br><span class="line">%  Furthermore, you will compute the training and test <span class="keyword">set</span> accuracies of </span><br><span class="line">%  our model.</span><br><span class="line">%</span><br><span class="line">%  Your task is to complete the code in predict.m</span><br><span class="line"></span><br><span class="line">%  Predict probability for a student with score 45 on exam 1 </span><br><span class="line">%  and score 85 on exam 2 </span><br><span class="line"></span><br><span class="line">prob = sigmoid([1 45 85] * theta);</span><br><span class="line">fprintf(['For a student with scores 45 and 85, we predict an admission ' ...</span><br><span class="line">         'probability of %f\n'], prob);</span><br><span class="line">fprintf('Expected value: 0.775 +/- 0.002\n\n');</span><br><span class="line"></span><br><span class="line">% Compute accuracy on our training <span class="keyword">set</span></span><br><span class="line">p = predict(theta, X);</span><br><span class="line"></span><br><span class="line">fprintf('Train Accuracy: %f\n', mean(double(p == y)) * 100);</span><br><span class="line">fprintf('Expected accuracy (approx): 89.0\n');</span><br><span class="line">fprintf('\n');</span><br></pre></td></tr></table></figure>
<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><h2 id="Part-1-Regularized-Logistic-Regression"><a href="#Part-1-Regularized-Logistic-Regression" class="headerlink" title="Part 1: Regularized Logistic Regression"></a>Part 1: Regularized Logistic Regression</h2><p><strong>costFunctionReg.m</strong><br>加入正则化项的代价函数和梯度<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">function [J, grad] = costFunctionReg(theta, X, y, lambda)</span><br><span class="line">%COSTFUNCTIONREG Compute cost and gradient <span class="keyword">for</span> logistic regression <span class="keyword">with</span> regularization</span><br><span class="line">%   J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost <span class="keyword">of</span> using</span><br><span class="line">%   theta <span class="keyword">as</span> the parameter <span class="keyword">for</span> regularized logistic regression and the</span><br><span class="line">%   gradient <span class="keyword">of</span> the cost w.r.t. to the parameters. </span><br><span class="line"></span><br><span class="line">% Initialize some useful values</span><br><span class="line">m = length(y); % number <span class="keyword">of</span> training examples</span><br><span class="line"></span><br><span class="line">% You need to <span class="keyword">return</span> the following variables correctly </span><br><span class="line">J = <span class="number">0</span>;</span><br><span class="line">grad = zeros(size(theta));</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the cost <span class="keyword">of</span> a particular choice <span class="keyword">of</span> theta.</span><br><span class="line">%               You should <span class="keyword">set</span> J to the cost.</span><br><span class="line">%               Compute the partial derivatives and <span class="keyword">set</span> grad to the partial</span><br><span class="line">%               derivatives of the cost w.r.t. each parameter in theta</span><br><span class="line"></span><br><span class="line">pos = y == 1;</span><br><span class="line">neg = y == 0;</span><br><span class="line"></span><br><span class="line">h_pos = sigmoid(X(pos, :) * theta);</span><br><span class="line">J_pos = sum(-log(h_pos));</span><br><span class="line"></span><br><span class="line">h_neg = sigmoid(X(neg, :) * theta);</span><br><span class="line">J_neg = sum(-log(1 - h_neg));</span><br><span class="line"></span><br><span class="line">J_reg = lambda/2 * sum(theta(2:end, :) .^ 2);</span><br><span class="line">J = (J_pos + J_neg + J_reg)/m;</span><br><span class="line"></span><br><span class="line">grad = (sum(X .* (sigmoid(X * theta) - y)))' / m;</span><br><span class="line">grad_reg = ((lambda * theta(2:end, :)) / m);</span><br><span class="line">grad(2:end, :) = grad(2:end, :) + grad_reg;</span><br><span class="line">% =============================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure></p>
<p><strong>运行结果</strong><br><img src="https://img-blog.csdnimg.cn/20190406153631739.png" width="60%" alt></p>
<pre><code>Cost at initial theta (zeros): 0.693147
Expected cost (approx): 0.693
Gradient at initial theta (zeros) - first five values only:
 0.008475 
 0.018788 
 0.000078 
 0.050345 
 0.011501 
Expected gradients (approx) - first five values only:
 0.0085
 0.0188
 0.0001
 0.0503
 0.0115

Program paused. Press enter to continue.

Cost at test theta (with lambda = 10): 3.164509
Expected cost (approx): 3.16
Gradient at test theta - first five values only:
 0.346045 
 0.161352 
 0.194796 
 0.226863 
 0.092186 
Expected gradients (approx) - first five values only:
 0.3460
 0.1614
 0.1948
 0.2269
 0.0922

Program paused. Press enter to continue.
</code></pre><h2 id="Part-2-Regularization-and-Accuracies"><a href="#Part-2-Regularization-and-Accuracies" class="headerlink" title="Part 2: Regularization and Accuracies"></a>Part 2: Regularization and Accuracies</h2><p><strong>运行结果</strong></p>
<pre><code>Train Accuracy: 83.050847
Expected accuracy (with lambda = 1): 83.1 (approx)
</code></pre><p><img src="https://img-blog.csdnimg.cn/20190406161056600.png" width="60%" alt></p>
<h2 id="主函数代码-1"><a href="#主函数代码-1" class="headerlink" title="主函数代码"></a>主函数代码</h2><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"> %% Machine Learning Online Class - Exercise <span class="number">2</span>: Logistic Regression</span><br><span class="line">%</span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">%</span><br><span class="line">%  This file contains code that helps you <span class="keyword">get</span> started on the second part</span><br><span class="line">%  of the exercise which covers regularization with logistic regression.</span><br><span class="line">%</span><br><span class="line">%  You will need to complete the following functions in this exericse:</span><br><span class="line">%</span><br><span class="line">%     sigmoid.m</span><br><span class="line">%     costFunction.m</span><br><span class="line">%     predict.m</span><br><span class="line">%     costFunctionReg.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">%% Load Data</span><br><span class="line">%  The first two columns contains the X values and the third column</span><br><span class="line">%  contains the label (y).</span><br><span class="line"></span><br><span class="line">data = load('ex2data2.txt');</span><br><span class="line">X = data(:, [1, 2]); y = data(:, 3);</span><br><span class="line"></span><br><span class="line">plotData(X, y);</span><br><span class="line"></span><br><span class="line">% Put some labels</span><br><span class="line">hold on;</span><br><span class="line"></span><br><span class="line">% Labels and Legend</span><br><span class="line">xlabel('Microchip Test 1')</span><br><span class="line">ylabel('Microchip Test 2')</span><br><span class="line"></span><br><span class="line">% Specified in plot order</span><br><span class="line">legend('y = 1', 'y = 0')</span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% =========== Part 1: Regularized Logistic Regression ============</span><br><span class="line">%  In this part, you are given a dataset with data points that are not</span><br><span class="line">%  linearly separable. However, you would still like to use logistic</span><br><span class="line">%  regression to classify the data points.</span><br><span class="line">%</span><br><span class="line">%  To do so, you introduce more features to use -- in particular, you add</span><br><span class="line">%  polynomial features to our data matrix (similar to polynomial</span><br><span class="line">%  regression).</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Add Polynomial Features</span><br><span class="line"></span><br><span class="line">% Note that mapFeature also adds a column of ones for us, so the intercept</span><br><span class="line">% term is handled</span><br><span class="line">X = mapFeature(X(:,1), X(:,2));</span><br><span class="line"></span><br><span class="line">% Initialize fitting parameters</span><br><span class="line">initial_theta = zeros(size(X, 2), 1);</span><br><span class="line"></span><br><span class="line">% Set regularization parameter lambda to 1</span><br><span class="line">lambda = 1;</span><br><span class="line"></span><br><span class="line">% Compute and display initial cost and gradient for regularized logistic</span><br><span class="line">% regression</span><br><span class="line">[cost, grad] = costFunctionReg(initial_theta, X, y, lambda);</span><br><span class="line"></span><br><span class="line">fprintf('Cost at initial theta (zeros): %f\n', cost);</span><br><span class="line">fprintf('Expected cost (approx): 0.693\n');</span><br><span class="line">fprintf('Gradient at initial theta (zeros) - first five values only:\n');</span><br><span class="line">fprintf(' %f \n', grad(1:5));</span><br><span class="line">fprintf('Expected gradients (approx) - first five values only:\n');</span><br><span class="line">fprintf(' 0.0085\n 0.0188\n 0.0001\n 0.0503\n 0.0115\n');</span><br><span class="line"></span><br><span class="line">fprintf('\nProgram paused. Press enter to continue.\n');</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">% Compute and display cost and gradient</span><br><span class="line">% with all-ones theta and lambda = 10</span><br><span class="line">test_theta = ones(size(X,2),1);</span><br><span class="line">[cost, grad] = costFunctionReg(test_theta, X, y, 10);</span><br><span class="line"></span><br><span class="line">fprintf('\nCost at test theta (with lambda = 10): %f\n', cost);</span><br><span class="line">fprintf('Expected cost (approx): 3.16\n');</span><br><span class="line">fprintf('Gradient at test theta - first five values only:\n');</span><br><span class="line">fprintf(' %f \n', grad(1:5));</span><br><span class="line">fprintf('Expected gradients (approx) - first five values only:\n');</span><br><span class="line">fprintf(' 0.3460\n 0.1614\n 0.1948\n 0.2269\n 0.0922\n');</span><br><span class="line"></span><br><span class="line">fprintf('\nProgram paused. Press enter to continue.\n');</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ============= Part 2: Regularization and Accuracies =============</span><br><span class="line">%  Optional Exercise:</span><br><span class="line">%  In this part, you will <span class="keyword">get</span> to try different values of lambda and</span><br><span class="line">%  see how regularization affects the decision coundart</span><br><span class="line">%</span><br><span class="line">%  Try the following values of lambda (0, 1, 10, 100).</span><br><span class="line">%</span><br><span class="line">%  How does the decision boundary change when you vary lambda? How does</span><br><span class="line">%  the training <span class="keyword">set</span> accuracy vary?</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">% Initialize fitting parameters</span><br><span class="line">initial_theta = zeros(size(X, 2), 1);</span><br><span class="line"></span><br><span class="line">% Set regularization parameter lambda to 1 (you should vary this)</span><br><span class="line">lambda = 1;</span><br><span class="line"></span><br><span class="line">% Set Options</span><br><span class="line">options = optimset('GradObj', 'on', 'MaxIter', 400);</span><br><span class="line"></span><br><span class="line">% Optimize</span><br><span class="line">[theta, J, exit_flag] = ...</span><br><span class="line">	fminunc(@(t)(costFunctionReg(t, X, y, lambda)), initial_theta, options);</span><br><span class="line"></span><br><span class="line">% Plot Boundary</span><br><span class="line">plotDecisionBoundary(theta, X, y);</span><br><span class="line">hold on;</span><br><span class="line">title(sprintf('lambda = %g', lambda))</span><br><span class="line"></span><br><span class="line">% Labels and Legend</span><br><span class="line">xlabel('Microchip Test 1')</span><br><span class="line">ylabel('Microchip Test 2')</span><br><span class="line"></span><br><span class="line">legend('y = 1', 'y = 0', 'Decision boundary')</span><br><span class="line">hold off;</span><br><span class="line"></span><br><span class="line">% Compute accuracy on our training <span class="keyword">set</span></span><br><span class="line">p = predict(theta, X);</span><br><span class="line"></span><br><span class="line">fprintf('Train Accuracy: %f\n', mean(double(p == y)) * 100);</span><br><span class="line">fprintf('Expected accuracy (with lambda = 1): 83.1 (approx)\n');</span><br></pre></td></tr></table></figure>
<blockquote>
<p>实验二完成</p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://loyf.github.io/2019/04/16/吴恩达机器学习实验一完整代码/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Loy Fan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Loy With No Title">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/16/吴恩达机器学习实验一完整代码/" itemprop="url">吴恩达机器学习实验一完整代码</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-16T20:53:53+08:00">
                2019-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<p>@<a href="https://loyf.github.io/">LoyFan</a></p>
<blockquote>
<p><a href="https://study.163.com/course/courseMain.htm?courseId=1004570029&amp;_trace_c_p_k2_=ea0c7bf2c97246f08030b6c79f38e69a" target="_blank" rel="noopener">&gt;吴恩达机器学习课程链接</a><br><a href="https://blog.csdn.net/weixin_43318626/article/details/88896788" target="_blank" rel="noopener">&gt;课程总结和笔记链接</a><br>实验一的原始代码和使用数据可至课程链接-课时45-章节6编程作业中下载</p>
<p>包括热身练习、单变量/多变量的损失函数计算、梯度下降的参数更新、特征归一化、正规方程等<br>环境——Matlab R2018b/Octave</p>
</blockquote>
<h1 id="单特征"><a href="#单特征" class="headerlink" title="单特征"></a>单特征</h1><h2 id="Part-1-Basic-Function"><a href="#Part-1-Basic-Function" class="headerlink" title="Part 1: Basic Function"></a>Part 1: Basic Function</h2><p>输出一个5X5单位矩阵</p>
<p><strong>warmUpExercise.m</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">A</span> = <span class="title">warmUpExercise</span>(<span class="params"></span>)</span></span><br><span class="line">%WARMUPEXERCISE Example function in octave</span><br><span class="line">%   A = WARMUPEXERCISE() is an example <span class="function"><span class="keyword">function</span> <span class="title">that</span> <span class="title">returns</span> <span class="title">the</span> 5<span class="title">x5</span> <span class="title">identity</span> <span class="title">matrix</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line">A = [];</span><br><span class="line">% ============= YOUR CODE HERE ==============</span><br><span class="line">% Instructions: Return the <span class="number">5</span>x5 identity matrix </span><br><span class="line">%               In octave, we <span class="keyword">return</span> values by defining which variables</span><br><span class="line">%               represent the <span class="keyword">return</span> values (at the top <span class="keyword">of</span> the file)</span><br><span class="line">%               and then <span class="keyword">set</span> them accordingly. </span><br><span class="line"></span><br><span class="line">A = eye(5);</span><br><span class="line"></span><br><span class="line">% ===========================================</span><br><span class="line">end</span><br></pre></td></tr></table></figure></p>
<p><strong>运行结果</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20190401142455241.png" width="40%" alt></p>
<h2 id="Part-2-Plotting"><a href="#Part-2-Plotting" class="headerlink" title="Part 2: Plotting"></a>Part 2: Plotting</h2><p>显示数据集ex1data1.txt</p>
<p><strong>plotData.m</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">plotData</span>(<span class="params">x, y</span>)</span></span><br><span class="line">%PLOTDATA Plots the data points x and y into a new figure </span><br><span class="line">%   PLOTDATA(x,y) plots the data points and gives the figure axes labels <span class="keyword">of</span></span><br><span class="line">%   population and profit.</span><br><span class="line"></span><br><span class="line">figure; % open a <span class="keyword">new</span> figure <span class="built_in">window</span></span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Plot the training data into a figure using the </span><br><span class="line">%               <span class="string">"figure"</span> and <span class="string">"plot"</span> commands. Set the axes labels using</span><br><span class="line">%               the <span class="string">"xlabel"</span> and <span class="string">"ylabel"</span> commands. Assume the </span><br><span class="line">%               population and revenue data have been passed <span class="keyword">in</span></span><br><span class="line">%               <span class="keyword">as</span> the x and y <span class="built_in">arguments</span> <span class="keyword">of</span> <span class="keyword">this</span> <span class="function"><span class="keyword">function</span>.</span></span><br><span class="line">%</span><br><span class="line">% Hint: You can use the <span class="string">'rx'</span> option <span class="keyword">with</span> plot to have the markers</span><br><span class="line">%       appear <span class="keyword">as</span> red crosses. Furthermore, you can make the</span><br><span class="line">%       markers larger by using plot(..., <span class="string">'rx'</span>, <span class="string">'MarkerSize'</span>, <span class="number">10</span>);</span><br><span class="line"></span><br><span class="line">plot(x, y, <span class="string">'rx'</span>, <span class="string">'MarkerSize'</span>, <span class="number">10</span>);</span><br><span class="line">xlabel(<span class="string">'population'</span>);</span><br><span class="line">ylabel(<span class="string">'revenue'</span>);</span><br><span class="line">% ============================================================</span><br><span class="line">end</span><br></pre></td></tr></table></figure></p>
<p><strong>运行结果</strong><br><img src="https://img-blog.csdnimg.cn/20190401142839796.png" width="60%" alt></p>
<h2 id="Part-3-Cost-and-Gradient-descent"><a href="#Part-3-Cost-and-Gradient-descent" class="headerlink" title="Part 3: Cost and Gradient descent"></a>Part 3: Cost and Gradient descent</h2><p>损失函数和梯度下降</p>
<p><strong>computeCost.m</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">J</span> = <span class="title">computeCost</span>(<span class="params">X, y, theta</span>)</span></span><br><span class="line">%COMPUTECOST Compute cost for linear regression</span><br><span class="line">%   J = COMPUTECOST(X, y, theta) computes the cost <span class="keyword">of</span> using theta <span class="keyword">as</span> the</span><br><span class="line">%   parameter <span class="keyword">for</span> linear regression to fit the data points <span class="keyword">in</span> X and y</span><br><span class="line"></span><br><span class="line">% Initialize some useful values</span><br><span class="line">m = length(y); % number <span class="keyword">of</span> training examples</span><br><span class="line"></span><br><span class="line">% You need to <span class="keyword">return</span> the following variables correctly </span><br><span class="line">J = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Compute the cost <span class="keyword">of</span> a particular choice <span class="keyword">of</span> theta</span><br><span class="line">%               You should <span class="keyword">set</span> J to the cost.</span><br><span class="line"></span><br><span class="line">predictions = X * theta;</span><br><span class="line">sqrErrors = (predictions - y).^2;</span><br><span class="line">J = 1 / (2 * m) * sum(sqrErrors);</span><br><span class="line"></span><br><span class="line">% =========================================================================</span><br><span class="line">end</span><br></pre></td></tr></table></figure></p>
<p><strong>gradientDescent.m</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)</span><br><span class="line">%GRADIENTDESCENT Performs gradient descent to learn theta</span><br><span class="line">%   theta = GRADIENTDESCENT(X, y, theta, alpha, num_iters) updates theta by </span><br><span class="line">%   taking num_iters gradient steps <span class="keyword">with</span> learning rate alpha</span><br><span class="line"></span><br><span class="line">% Initialize some useful values</span><br><span class="line">m = length(y); % number <span class="keyword">of</span> training examples</span><br><span class="line">J_history = zeros(num_iters, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:num_iters</span><br><span class="line"></span><br><span class="line">    % ====================== YOUR CODE HERE ======================</span><br><span class="line">    % Instructions: Perform a single gradient step on the parameter vector</span><br><span class="line">    %               theta. </span><br><span class="line">    %</span><br><span class="line">    % Hint: While debugging, it can be useful to print out the values</span><br><span class="line">    %       <span class="keyword">of</span> the cost <span class="function"><span class="keyword">function</span> (<span class="params">computeCost</span>) <span class="title">and</span> <span class="title">gradient</span> <span class="title">here</span>.</span></span><br><span class="line">    %</span><br><span class="line">    predictions0 = <span class="number">1</span> / m * sum(X * theta - y);</span><br><span class="line">    theta(<span class="number">1</span>) = theta(<span class="number">1</span>) - alpha * predictions0;</span><br><span class="line">    predictions1 = <span class="number">1</span> / m * sum((X * theta - y) .* X(:,<span class="number">2</span>));</span><br><span class="line">    theta(<span class="number">2</span>) = theta(<span class="number">2</span>) - alpha * predictions1;</span><br><span class="line"></span><br><span class="line">    % ============================================================</span><br><span class="line"></span><br><span class="line">    % Save the cost J <span class="keyword">in</span> every iteration    </span><br><span class="line">    J_history(iter) = computeCost(X, y, theta);</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure></p>
<p><strong>运行结果</strong><br><img src="https://img-blog.csdnimg.cn/20190401143937749.png" width="80%" alt><br><img src="https://img-blog.csdnimg.cn/20190401143951120.png" width="60%" alt></p>
<h2 id="Part-4-Visualizing-J-theta-0-theta-1"><a href="#Part-4-Visualizing-J-theta-0-theta-1" class="headerlink" title="Part 4: Visualizing J(theta_0, theta_1)"></a>Part 4: Visualizing J(theta_0, theta_1)</h2><p>参数更新视图<br><img src="https://img-blog.csdnimg.cn/20190401144134784.png" width="55%" alt><br><img src="https://img-blog.csdnimg.cn/20190401144143359.png" width="60%" alt></p>
<h2 id="主函数代码"><a href="#主函数代码" class="headerlink" title="主函数代码"></a>主函数代码</h2><p><strong>ex1.m</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class - Exercise <span class="number">1</span>: Linear Regression</span><br><span class="line"></span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">%</span><br><span class="line">%  This file contains code that helps you <span class="keyword">get</span> started on the</span><br><span class="line">%  linear exercise. You will need to complete the following functions</span><br><span class="line">%  in this exericse:</span><br><span class="line">%</span><br><span class="line">%     warmUpExercise.m</span><br><span class="line">%     plotData.m</span><br><span class="line">%     gradientDescent.m</span><br><span class="line">%     computeCost.m</span><br><span class="line">%     gradientDescentMulti.m</span><br><span class="line">%     computeCostMulti.m</span><br><span class="line">%     featureNormalize.m</span><br><span class="line">%     normalEqn.m</span><br><span class="line">%</span><br><span class="line">%  For this exercise, you will not need to change any code in this file,</span><br><span class="line">%  or any other files other than those mentioned above.</span><br><span class="line">%</span><br><span class="line">% x refers to the population size in 10,000s</span><br><span class="line">% y refers to the profit in $10,000s</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">%% ==================== Part 1: Basic Function ====================</span><br><span class="line">% Complete warmUpExercise.m</span><br><span class="line">fprintf('Running warmUpExercise ... \n');</span><br><span class="line">fprintf('5x5 Identity Matrix: \n');</span><br><span class="line">warmUpExercise()</span><br><span class="line"></span><br><span class="line">fprintf('Program paused. Press enter to continue.\n');</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ======================= Part 2: Plotting =======================</span><br><span class="line">fprintf('Plotting Data ...\n')</span><br><span class="line">data = load('ex1data1.txt');</span><br><span class="line">X = data(:, 1); y = data(:, 2);</span><br><span class="line">m = length(y); % number of training examples</span><br><span class="line"></span><br><span class="line">% Plot Data</span><br><span class="line">% Note: You have to complete the code in plotData.m</span><br><span class="line">plotData(X, y);</span><br><span class="line"></span><br><span class="line">fprintf('Program paused. Press enter to continue.\n');</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% =================== Part 3: Cost and Gradient descent ===================</span><br><span class="line"></span><br><span class="line">X = [ones(m, 1), data(:,1)]; % Add a column of ones to x</span><br><span class="line">theta = zeros(2, 1); % initialize fitting parameters</span><br><span class="line"></span><br><span class="line">% Some gradient descent settings</span><br><span class="line">iterations = 1500;</span><br><span class="line">alpha = 0.01;</span><br><span class="line"></span><br><span class="line">fprintf('\nTesting the cost function ...\n')</span><br><span class="line">% compute and display initial cost</span><br><span class="line">J = computeCost(X, y, theta);</span><br><span class="line">fprintf('With theta = [0 ; 0]\nCost computed = %f\n', J);</span><br><span class="line">fprintf('Expected cost value (approx) 32.07\n');</span><br><span class="line"></span><br><span class="line">% further testing of the cost function</span><br><span class="line">J = computeCost(X, y, [-1 ; 2]);</span><br><span class="line">fprintf('\nWith theta = [-1 ; 2]\nCost computed = %f\n', J);</span><br><span class="line">fprintf('Expected cost value (approx) 54.24\n');</span><br><span class="line"></span><br><span class="line">fprintf('Program paused. Press enter to continue.\n');</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">fprintf('\nRunning Gradient Descent ...\n')</span><br><span class="line">% run gradient descent</span><br><span class="line">theta = gradientDescent(X, y, theta, alpha, iterations);</span><br><span class="line"></span><br><span class="line">% print theta to screen</span><br><span class="line">fprintf('Theta found by gradient descent:\n');</span><br><span class="line">fprintf('%f\n', theta);</span><br><span class="line">fprintf('Expected theta values (approx)\n');</span><br><span class="line">fprintf(' -3.6303\n  1.1664\n\n');</span><br><span class="line"></span><br><span class="line">% Plot the linear fit</span><br><span class="line">hold on; % keep previous plot visible</span><br><span class="line">plot(X(:,2), X*theta, '-')</span><br><span class="line">legend('Training data', 'Linear regression')</span><br><span class="line">hold off % don't overlay any more plots on this figure</span><br><span class="line"></span><br><span class="line">% Predict values for population sizes of 35,000 and 70,000</span><br><span class="line">predict1 = [1, 3.5] *theta;</span><br><span class="line">fprintf('For population = 35,000, we predict a profit of %f\n',...</span><br><span class="line">    predict1*10000);</span><br><span class="line">predict2 = [1, 7] * theta;</span><br><span class="line">fprintf('For population = 70,000, we predict a profit of %f\n',...</span><br><span class="line">    predict2*10000);</span><br><span class="line"></span><br><span class="line">fprintf('Program paused. Press enter to continue.\n');</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ============= Part 4: Visualizing J(theta_0, theta_1) =============</span><br><span class="line">fprintf('Visualizing J(theta_0, theta_1) ...\n')</span><br><span class="line"></span><br><span class="line">% Grid over which we will calculate J</span><br><span class="line">theta0_vals = linspace(-10, 10, 100);</span><br><span class="line">theta1_vals = linspace(-1, 4, 100);</span><br><span class="line"></span><br><span class="line">% initialize J_vals to a matrix of 0's</span><br><span class="line">J_vals = zeros(length(theta0_vals), length(theta1_vals));</span><br><span class="line"></span><br><span class="line">% Fill out J_vals</span><br><span class="line">for i = 1:length(theta0_vals)</span><br><span class="line">    for j = 1:length(theta1_vals)</span><br><span class="line">	  t = [theta0_vals(i); theta1_vals(j)];</span><br><span class="line">	  J_vals(i,j) = computeCost(X, y, t);</span><br><span class="line">    end</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% Because of the way meshgrids work in the surf command, we need to</span><br><span class="line">% transpose J_vals before calling surf, or else the axes will be flipped</span><br><span class="line">J_vals = J_vals';</span><br><span class="line">% Surface plot</span><br><span class="line">figure;</span><br><span class="line">surf(theta0_vals, theta1_vals, J_vals)</span><br><span class="line">xlabel('\theta_0'); ylabel('\theta_1');</span><br><span class="line"></span><br><span class="line">% Contour plot</span><br><span class="line">figure;</span><br><span class="line">% Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100</span><br><span class="line">contour(theta0_vals, theta1_vals, J_vals, logspace(-2, 3, 20))</span><br><span class="line">xlabel('\theta_0'); ylabel('\theta_1');</span><br><span class="line">hold on;</span><br><span class="line">plot(theta(1), theta(2), 'rx', 'MarkerSize', 10, 'LineWidth', 2);</span><br></pre></td></tr></table></figure></p>
<h1 id="多特征"><a href="#多特征" class="headerlink" title="多特征"></a>多特征</h1><h2 id="Part-1-Feature-Normalization"><a href="#Part-1-Feature-Normalization" class="headerlink" title="Part 1: Feature Normalization"></a>Part 1: Feature Normalization</h2><p>多元特征归一化</p>
<p><strong>featureNormalize.m</strong><br>使得特征均值为0，标准差为1</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">function [X_norm, mu, sigma] = featureNormalize(X)</span><br><span class="line">%FEATURENORMALIZE Normalizes the features <span class="keyword">in</span> X </span><br><span class="line">%   FEATURENORMALIZE(X) returns a normalized version <span class="keyword">of</span> X where</span><br><span class="line">%   the mean value <span class="keyword">of</span> each feature is <span class="number">0</span> and the standard deviation</span><br><span class="line">%   is <span class="number">1.</span> This is often a good preprocessing step to <span class="keyword">do</span> when</span><br><span class="line">%   working <span class="keyword">with</span> learning algorithms.</span><br><span class="line"></span><br><span class="line">% You need to <span class="keyword">set</span> these values correctly</span><br><span class="line"></span><br><span class="line">X_norm = X;</span><br><span class="line">mu = zeros(1, size(X, 2));</span><br><span class="line">sigma = zeros(1, size(X, 2));</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: First, for each feature dimension, compute the mean</span><br><span class="line">%               of the feature and subtract it from the dataset,</span><br><span class="line">%               storing the mean value in mu. Next, compute the </span><br><span class="line">%               standard deviation of each feature and divide</span><br><span class="line">%               each feature by it's standard deviation, storing</span><br><span class="line">%               the standard deviation in sigma. </span><br><span class="line">%</span><br><span class="line">%               Note that X is a matrix where each column is a </span><br><span class="line">%               feature and each row is an example. You need </span><br><span class="line">%               to perform the normalization separately for </span><br><span class="line">%               each feature. </span><br><span class="line">%</span><br><span class="line">% Hint: You might find the 'mean' and 'std' functions useful.</span><br><span class="line">%       </span><br><span class="line"></span><br><span class="line">mu = mean(X_norm, 1);</span><br><span class="line">sigma = std(X_norm, 0, 1);   %std(X, OPT, dim) X待计算矩阵，normalization OPT must be 0 or 1，dim以哪个维度计算（行/列）</span><br><span class="line">X_norm = (X_norm - mu) ./ sigma;</span><br><span class="line"></span><br><span class="line">%standard_deviation = std(X_norm, 0, 1);</span><br><span class="line">%Mean = mean(X_norm, 1);</span><br><span class="line">%fprintf('mean value: \n');</span><br><span class="line">%fprintf(' %.2f\n', [Mean(1:1,:)]);</span><br><span class="line">%fprintf('standard deviation: \n');</span><br><span class="line">%fprintf(' %.2f\n', [standard_deviation(1:1,:)]);</span><br><span class="line"></span><br><span class="line">% ============================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h2 id="Part-2-Gradient-Descent"><a href="#Part-2-Gradient-Descent" class="headerlink" title="Part 2: Gradient Descent"></a>Part 2: Gradient Descent</h2><p>多元损失函数和梯度下降</p>
<p><strong>computeCostMulti.m</strong><br>和上述的损失函数相同</p>
<p><strong>gradientDescentMulti.m</strong><br>和上述的单一变量梯度下降几乎相同<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">function [theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters)</span><br><span class="line">%GRADIENTDESCENTMULTI Performs gradient descent to learn theta</span><br><span class="line">%   theta = GRADIENTDESCENTMULTI(x, y, theta, alpha, num_iters) updates theta by</span><br><span class="line">%   taking num_iters gradient steps <span class="keyword">with</span> learning rate alpha</span><br><span class="line"></span><br><span class="line">% Initialize some useful values</span><br><span class="line">m = length(y); % number <span class="keyword">of</span> training examples</span><br><span class="line">J_history = zeros(num_iters, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:num_iters</span><br><span class="line"></span><br><span class="line">    % ====================== YOUR CODE HERE ======================</span><br><span class="line">    % Instructions: Perform a single gradient step on the parameter vector</span><br><span class="line">    %               theta. </span><br><span class="line">    %</span><br><span class="line">    % Hint: While debugging, it can be useful to print out the values</span><br><span class="line">    %       <span class="keyword">of</span> the cost <span class="function"><span class="keyword">function</span> (<span class="params">computeCostMulti</span>) <span class="title">and</span> <span class="title">gradient</span> <span class="title">here</span>.</span></span><br><span class="line">    %</span><br><span class="line"></span><br><span class="line">    predictions0 = <span class="number">1</span> / m * sum(X * theta - y);</span><br><span class="line">    theta(<span class="number">1</span>) = theta(<span class="number">1</span>) - alpha * predictions0;</span><br><span class="line">    predictions1 = <span class="number">1</span> / m * sum((X * theta - y) .* X(:, <span class="number">2</span>));</span><br><span class="line">    theta(<span class="number">2</span>) = theta(<span class="number">2</span>) - alpha * predictions1;    </span><br><span class="line">    predictions2 = <span class="number">1</span> / m * sum((X * theta - y) .* X(:, <span class="number">3</span>));</span><br><span class="line">    theta(<span class="number">3</span>) = theta(<span class="number">3</span>) - alpha * predictions2;</span><br><span class="line">   </span><br><span class="line">    % ============================================================</span><br><span class="line"></span><br><span class="line">    % Save the cost J <span class="keyword">in</span> every iteration</span><br><span class="line">    J_history(iter) = computeCostMulti(X, y, theta);</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure></p>
<h2 id="用梯度下降选择参数theta并且预测"><a href="#用梯度下降选择参数theta并且预测" class="headerlink" title="用梯度下降选择参数theta并且预测"></a>用梯度下降选择参数theta并且预测</h2><p>这段代码包含于主函数代码中<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">fprintf(<span class="string">'Running gradient descent ...\n'</span>);</span><br><span class="line"></span><br><span class="line">% Choose some alpha value</span><br><span class="line">alpha = <span class="number">0.01</span>;</span><br><span class="line">num_iters = <span class="number">400</span>;</span><br><span class="line"></span><br><span class="line">% Init Theta and Run Gradient Descent</span><br><span class="line">theta = zeros(<span class="number">3</span>, <span class="number">1</span>);</span><br><span class="line">[theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters);</span><br><span class="line"></span><br><span class="line">% Plot the convergence graph</span><br><span class="line">figure;</span><br><span class="line">plot(<span class="number">1</span>:numel(J_history), J_history, <span class="string">'-b'</span>, <span class="string">'LineWidth'</span>, <span class="number">2</span>);</span><br><span class="line">xlabel(<span class="string">'Number of iterations'</span>);</span><br><span class="line">ylabel(<span class="string">'Cost J'</span>);</span><br><span class="line"></span><br><span class="line">% Display gradient descent<span class="string">'s result</span></span><br><span class="line"><span class="string">fprintf('</span>Theta computed <span class="keyword">from</span> gradient descent: \n<span class="string">');</span></span><br><span class="line"><span class="string">fprintf('</span> %f \n<span class="string">', theta);</span></span><br><span class="line"><span class="string">fprintf('</span>\n<span class="string">');</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">% Estimate the price of a 1650 sq-ft, 3 br house</span></span><br><span class="line"><span class="string">% ====================== YOUR CODE HERE ======================</span></span><br><span class="line"><span class="string">% Recall that the first column of X is all-ones. Thus, it does</span></span><br><span class="line"><span class="string">% not need to be normalized.</span></span><br><span class="line"><span class="string">%price = 0; % You should change this</span></span><br><span class="line"><span class="string">x = [1650 3];</span></span><br><span class="line"><span class="string">x = (x - mu) ./ sigma;</span></span><br><span class="line"><span class="string">x = [ones(1, 1) x];</span></span><br><span class="line"><span class="string">price = x * theta;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">% ============================================================</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">fprintf(['</span>Predicted price <span class="keyword">of</span> a <span class="number">1650</span> sq-ft, <span class="number">3</span> br house <span class="string">' ...</span></span><br><span class="line"><span class="string">         '</span>(using gradient descent):\n $%f\n<span class="string">'], price);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">fprintf('</span>Program paused. Press enter to <span class="keyword">continue</span>.\n<span class="string">');</span></span><br><span class="line"><span class="string">pause;</span></span><br></pre></td></tr></table></figure></p>
<p><strong>运行结果</strong><br><img src="https://img-blog.csdnimg.cn/20190401205313219.png" width="60%" alt><br><img src="https://img-blog.csdnimg.cn/20190401205502808.png" width="60%" alt></p>
<h2 id="Part-3-Normal-Equations"><a href="#Part-3-Normal-Equations" class="headerlink" title="Part 3: Normal Equations"></a>Part 3: Normal Equations</h2><p>正规方程<br>非迭代方法选择参数，并做与上述相同的预测</p>
<p><strong>normalEqn.m</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">function [theta] = normalEqn(X, y)</span><br><span class="line">%NORMALEQN Computes the closed-form solution to linear regression </span><br><span class="line">%   NORMALEQN(X,y) computes the closed-form solution to linear </span><br><span class="line">%   regression using the normal equations.</span><br><span class="line"></span><br><span class="line">theta = zeros(size(X, <span class="number">2</span>), <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: Complete the code to compute the closed form solution</span><br><span class="line">%               to linear regression and put the result <span class="keyword">in</span> theta.</span><br><span class="line">%</span><br><span class="line">theta = pinv(X<span class="string">'*X)*X'</span>*y;</span><br><span class="line"></span><br><span class="line">% ============================================================</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure></p>
<p><strong>运行结果</strong><br><img src="https://img-blog.csdnimg.cn/20190401211028783.png" width="60%" alt></p>
<h2 id="主函数代码-1"><a href="#主函数代码-1" class="headerlink" title="主函数代码"></a>主函数代码</h2><p><strong>ex1_multi.m</strong><br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br></pre></td><td class="code"><pre><span class="line">%% Machine Learning Online Class</span><br><span class="line">%  Exercise <span class="number">1</span>: Linear regression <span class="keyword">with</span> multiple variables</span><br><span class="line">%</span><br><span class="line">%  Instructions</span><br><span class="line">%  ------------</span><br><span class="line">% </span><br><span class="line">%  This file contains code that helps you <span class="keyword">get</span> started on the</span><br><span class="line">%  linear regression exercise. </span><br><span class="line">%</span><br><span class="line">%  You will need to complete the following functions in this </span><br><span class="line">%  exericse:</span><br><span class="line">%</span><br><span class="line">%     warmUpExercise.m</span><br><span class="line">%     plotData.m</span><br><span class="line">%     gradientDescent.m</span><br><span class="line">%     computeCost.m</span><br><span class="line">%     gradientDescentMulti.m</span><br><span class="line">%     computeCostMulti.m</span><br><span class="line">%     featureNormalize.m</span><br><span class="line">%     normalEqn.m</span><br><span class="line">%</span><br><span class="line">%  For this part of the exercise, you will need to change some</span><br><span class="line">%  parts of the code below for various experiments (e.g., changing</span><br><span class="line">%  learning rates).</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Initialization</span><br><span class="line"></span><br><span class="line">%% ================ Part 1: Feature Normalization ================</span><br><span class="line"></span><br><span class="line">%% Clear and Close Figures</span><br><span class="line">clear ; close all; clc</span><br><span class="line"></span><br><span class="line">fprintf('Loading data ...\n');</span><br><span class="line"></span><br><span class="line">%% Load Data</span><br><span class="line">data = load('ex1data2.txt');</span><br><span class="line">X = data(:, 1:2);</span><br><span class="line">y = data(:, 3);</span><br><span class="line">m = length(y);</span><br><span class="line"></span><br><span class="line">% Print out some data points</span><br><span class="line">fprintf('First 10 examples from the dataset: \n');</span><br><span class="line">fprintf(' x = [%.0f %.0f], y = %.0f \n', [X(1:10,:) y(1:10,:)]');</span><br><span class="line"></span><br><span class="line">fprintf('Program paused. Press enter to continue.\n');</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">% Scale features and <span class="keyword">set</span> them to zero mean</span><br><span class="line">fprintf('Normalizing Features ...\n');</span><br><span class="line">[X, mu, sigma] = featureNormalize(X);</span><br><span class="line"></span><br><span class="line">% Add intercept term to X</span><br><span class="line">X = [ones(m, 1) X];</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%% ================ Part 2: Gradient Descent ================</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: We have provided you with the following starter</span><br><span class="line">%               code that runs gradient descent with a particular</span><br><span class="line">%               learning rate (alpha). </span><br><span class="line">%</span><br><span class="line">%               Your task is to first make sure that your functions - </span><br><span class="line">%               computeCost and gradientDescent already work with </span><br><span class="line">%               this starter code and support multiple variables.</span><br><span class="line">%</span><br><span class="line">%               After that, try running gradient descent with </span><br><span class="line">%               different values of alpha and see which one gives</span><br><span class="line">%               you the best result.</span><br><span class="line">%</span><br><span class="line">%               Finally, you should complete the code at the end</span><br><span class="line">%               to predict the price of a 1650 sq-ft, 3 br house.</span><br><span class="line">%</span><br><span class="line">% Hint: By using the 'hold on' command, you can plot multiple </span><br><span class="line">%       graphs on the same figure.</span><br><span class="line">%</span><br><span class="line">% Hint: At prediction, make sure you do the same feature normalization.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">fprintf('Running gradient descent ...\n');</span><br><span class="line"></span><br><span class="line">% Choose some alpha value</span><br><span class="line">alpha = 0.01;</span><br><span class="line">num_iters = 400;</span><br><span class="line"></span><br><span class="line">% Init Theta and Run Gradient Descent</span><br><span class="line">theta = zeros(3, 1);</span><br><span class="line">[theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters);</span><br><span class="line"></span><br><span class="line">% Plot the convergence graph</span><br><span class="line">figure;</span><br><span class="line">plot(1:numel(J_history), J_history, '-b', 'LineWidth', 2);</span><br><span class="line">xlabel('Number of iterations');</span><br><span class="line">ylabel('Cost J');</span><br><span class="line"></span><br><span class="line">% Display gradient descent's result</span><br><span class="line">fprintf('Theta computed from gradient descent: \n');</span><br><span class="line">fprintf(' %f \n', theta);</span><br><span class="line">fprintf('\n');</span><br><span class="line"></span><br><span class="line">% Estimate the price of a 1650 sq-ft, 3 br house</span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Recall that the first column of X is all-ones. Thus, it does</span><br><span class="line">% not need to be normalized.</span><br><span class="line">%price = 0; % You should change this</span><br><span class="line">x = [1650 3];</span><br><span class="line">x = (x - mu) ./ sigma;</span><br><span class="line">x = [ones(1, 1) x];</span><br><span class="line">price = x * theta;</span><br><span class="line"></span><br><span class="line">% ============================================================</span><br><span class="line"></span><br><span class="line">fprintf(['Predicted price of a 1650 sq-ft, 3 br house ' ...</span><br><span class="line">         '(using gradient descent):\n $%f\n'], price);</span><br><span class="line"></span><br><span class="line">fprintf('Program paused. Press enter to continue.\n');</span><br><span class="line">pause;</span><br><span class="line"></span><br><span class="line">%% ================ Part 3: Normal Equations ================</span><br><span class="line"></span><br><span class="line">fprintf('Solving with normal equations...\n');</span><br><span class="line"></span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">% Instructions: The following code computes the closed form </span><br><span class="line">%               solution for linear regression using the normal</span><br><span class="line">%               equations. You should complete the code in </span><br><span class="line">%               normalEqn.m</span><br><span class="line">%</span><br><span class="line">%               After doing so, you should complete this code </span><br><span class="line">%               to predict the price of a 1650 sq-ft, 3 br house.</span><br><span class="line">%</span><br><span class="line"></span><br><span class="line">%% Load Data</span><br><span class="line">data = csvread('ex1data2.txt');</span><br><span class="line">X = data(:, 1:2);</span><br><span class="line">y = data(:, 3);</span><br><span class="line">m = length(y);</span><br><span class="line"></span><br><span class="line">% Add intercept term to X</span><br><span class="line">X = [ones(m, 1) X];</span><br><span class="line"></span><br><span class="line">% Calculate the parameters from the normal equation</span><br><span class="line">theta = normalEqn(X, y);</span><br><span class="line"></span><br><span class="line">% Display normal equation's result</span><br><span class="line">fprintf('Theta computed from the normal equations: \n');</span><br><span class="line">fprintf(' %f \n', theta);</span><br><span class="line">fprintf('\n');</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">% Estimate the price of a 1650 sq-ft, 3 br house</span><br><span class="line">% ====================== YOUR CODE HERE ======================</span><br><span class="line">%price = 0; % You should change this</span><br><span class="line">x = [1650 3];</span><br><span class="line">x = [ones(1, 1) x];</span><br><span class="line">price = x * theta;</span><br><span class="line"></span><br><span class="line">% ============================================================</span><br><span class="line"></span><br><span class="line">fprintf(['Predicted price of a 1650 sq-ft, 3 br house ' ...</span><br><span class="line">         '(using normal equations):\n $%f\n'], price);</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>实验一完成</p>
</blockquote>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://loyf.github.io/2019/03/29/AlexNet-2012/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Loy Fan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Loy With No Title">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/29/AlexNet-2012/" itemprop="url">AlexNet-2012</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-29T16:51:57+08:00">
                2019-03-29
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <a id="more"></a>
<blockquote>
<p>NIPS 2012 《ImageNet Classification with Deep Convolutional Neural Networks》<br>Alex Krizhevsky | Ilya Sutskever | Geoffrey E. Hinton<br><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">PaperLink</a></p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/20190329141116436.png" width="100%" alt></p>
<p><img src="https://img-blog.csdnimg.cn/20190329200929967.png" width="100%" alt></p>
<h1 id="基本参数"><a href="#基本参数" class="headerlink" title="基本参数"></a>基本参数</h1><p><strong>input：</strong> 224×224大小的图片，3通道<br><strong>conv1：</strong> 11×11大小的卷积核96个，每个GPU上48个。<br><strong>max-pooling：</strong> 2×2的核。<br><strong>conv2：</strong> 5×5卷积核256个，每个GPU上128个。<br><strong>max-pooling：</strong> 2×2的核。<br><strong>conv3：</strong> 与上一层是全连接，3x3的卷积核384个。分到两个GPU上个192个。<br><strong>conv4：</strong> 3×3的卷积核384个，两个GPU各192个。该层与上一层连接没有经过pooling层。<br><strong>conv5：</strong> 3×3的卷积核256个，两个GPU上个128个。<br><strong>max-pooling：</strong> 2×2的核。<br><strong>FullyConnected1：</strong> 4096维，将第五层max-pooling的输出连接成为一个一维向量，作为该层的输入。<br><strong>FullyConnected2：</strong> 4096维<br><strong>Softmax：</strong> 输出为1000，输出的每一维都是图片属于该类别的概率。</p>
<h1 id="一些操作"><a href="#一些操作" class="headerlink" title="一些操作"></a>一些操作</h1><p><strong>ReLU</strong><br>使用非线性饱和函数ReLU作为神经元激活函数$f(x)=max(0,x)$</p>
<p><strong>数据增强</strong></p>
<ul>
<li>随机抓取224x224的小块，以及它的水平翻转：<br>从256x256的图片中抓取224x224的小块，并用这抓取的小块来训练网络；<br>使训练集增加了2048倍，但是样本间有高度依赖性；<br>不使用这个方案时，出现大量的过拟合；<br>测试阶段时，抓取5个224x224的小块以及它们的水平翻转（共10个）来做预测，并对这10个小块的softmax预测值做平均。</li>
<li>改变训练图像中的RGB通道的强度：<br>遍历ImageNet训练集，在RGB像素值的集合上使用PCA<br>使已知的主成分加倍<br>比例为对应特征值乘以一个随机变量<br>随机变量服从均值为0，标准差为0.1的高斯分布</li>
</ul>
<p><strong>Dual GPU</strong><br>使用了两个NVIDIA GTX 580 3GB GPU<br>每个GPU有一半的kernel，且只在一些特定的层，GPU之间才进行通信：</p>
<ul>
<li>第2、4、5卷积层的输入只连接了位于同一GPU的前一层的kernel；</li>
<li>第3层连接了第2层所有kernel（两个GPU的）；</li>
<li>全连接层是与前一层所有神经元连接的（两个GPU的）。</li>
</ul>
<p><strong>LRN(Local Response Normalization)局部响应归一化</strong><br><img src="https://img-blog.csdnimg.cn/20190329161627716.png" width="50%" alt></p>
<p>作者发现进行局部的标准化可以提高性能。</p>
<ul>
<li>其中a代表在feature map中第i个卷积核(x,y)坐标经过了ReLU激活函数的输出，n表示相邻的几个卷积核。N表示这一层总的卷积核数量。k, n, α和β是hyper-parameters，他们的值是在验证集上实验得到的，其中k = 2，n = 5，α = 0.0001，β = 0.75。</li>
<li>这种归一化操作实现了某种形式的横向抑制，这也是受真实神经元的某种行为启发。</li>
<li>卷积核矩阵的排序是随机任意，并且在训练之前就已经决定好顺序。这种LPN形成了一种横向抑制机制。</li>
</ul>
<p><strong>Overlap Pooling</strong><br>在池化层提取结果时，会受到相邻pooling单元的影响，有可能结果重复，即相邻池化单元有重叠部分，实验结果表明这比传统池化要好，在Top-1和Top-5上分别提高了0.4%和0.3%。</p>
<p><strong>Drop Out</strong><br>文章以0.5的概率将每个隐层的神经元输出置为0，dropout减少了神经元互适应性的复杂度，隐层中任何一个神经元都可能会以0.5的概率被丢弃，神经元无法依赖于其他特定的神经元而存在，因此这会迫使网络学习更为健壮、鲁棒的特征。运用了这种机制的神经元不会干扰前向传递也不影响后续操作。dropout减少了过拟合，也使收敛迭代次数增加一倍。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Loy Fan</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Loy Fan</span>

  
</div>








<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">28.1k words in this blog site.</span>
</div>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
