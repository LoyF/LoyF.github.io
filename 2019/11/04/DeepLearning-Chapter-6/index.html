<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Machine Learning,">










<meta name="description" content="@LoyFan">
<meta name="keywords" content="Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="DeepLearning Chapter 6">
<meta property="og:url" content="https://loyf.github.io/2019/11/04/DeepLearning-Chapter-6/index.html">
<meta property="og:site_name" content="Loy Fan">
<meta property="og:description" content="@LoyFan">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191102171006216.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2019110320464335.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191103205157560.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191104113314736.png">
<meta property="og:updated_time" content="2019-11-04T06:42:02.938Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DeepLearning Chapter 6">
<meta name="twitter:description" content="@LoyFan">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20191102171006216.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://loyf.github.io/2019/11/04/DeepLearning-Chapter-6/">





  <title>DeepLearning Chapter 6 | Loy Fan</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Loy Fan</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Life can not be planned</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://loyf.github.io/2019/11/04/DeepLearning-Chapter-6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Loy Fan">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Loy Fan">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">DeepLearning Chapter 6</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-11-04T14:15:24+08:00">
                2019-11-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Machine Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>@<a href="https://loyf.github.io/">LoyFan</a><br><a id="more"></a><br>mathjax:true</p>
<h1 id="深度前馈网络"><a href="#深度前馈网络" class="headerlink" title="深度前馈网络"></a>深度前馈网络</h1><h2 id="1-实例：学习XOR"><a href="#1-实例：学习XOR" class="headerlink" title="1 实例：学习XOR"></a>1 实例：学习XOR</h2><h2 id="2-基于梯度的学习"><a href="#2-基于梯度的学习" class="headerlink" title="2 基于梯度的学习"></a>2 基于梯度的学习</h2><h3 id="2-2-输出单元"><a href="#2-2-输出单元" class="headerlink" title="2.2 输出单元"></a>2.2 输出单元</h3><p>输出单元即网络最终层得到的结果。输出单元的选择影响了代价函数的选择。<br>原则上用作输出单元的神经网络单元也可以用作隐藏层的单元。<br>在本节中，我们假设前馈网络提供了一组定义为 $h = f(x; θ)$ 的隐藏特征。输出层的作用是随后对这些特征进行一些额外的变换来完成整个网络必须完成的任务。</p>
<h4 id="2-2-1-用于高斯输出分布的线性单元"><a href="#2-2-1-用于高斯输出分布的线性单元" class="headerlink" title="2.2.1 用于高斯输出分布的线性单元"></a>2.2.1 用于高斯输出分布的线性单元</h4><p>给定特征 $\boldsymbol{h}$，线性输出单元层产生向量</p>
<script type="math/tex; mode=display">
\hat{\boldsymbol{y}}=\boldsymbol{W}^{\top}\boldsymbol{h}+\boldsymbol{b}</script><p>线性输出层经常被用来产生条件高斯分布的均值：</p>
<script type="math/tex; mode=display">
p(\boldsymbol{y} | \boldsymbol{x})=\mathcal{N}(\boldsymbol{y} ; \hat{\boldsymbol{y}}, \boldsymbol{I})</script><p>最大化条件高斯分布均值的对数似然等价于最小化均方误差。<br>因为线性模型不会饱和，所以它们易于采用基于梯度的优化算法，甚至可以使用其他多种优化算法。</p>
<h4 id="2-2-2-用于-Bernoulli-输出分布的-sigmoid-单元"><a href="#2-2-2-用于-Bernoulli-输出分布的-sigmoid-单元" class="headerlink" title="2.2.2 用于 Bernoulli 输出分布的 sigmoid 单元"></a>2.2.2 用于 Bernoulli 输出分布的 sigmoid 单元</h4><p>许多任务需要预测二值型变量，比如两个类别的分类问题。<br>此时最大似然的定义是 $y$在 $\boldsymbol{x}$下的Bernoulli分布<br>神经网络只需要预测 $P(y = 1 | x)$ 即可。<br>为了使这个数是有效的概率，它必须处在区间 [0,  1] 中。<br>假设采用：</p>
<script type="math/tex; mode=display">
P(y=1 | x)=\max \left\{0, \min \left\{1, \boldsymbol{w}^{\top} \boldsymbol{h}+b\right\}\right\}</script><p>它是一个有效的条件概率分布，但我们无法使用梯度下降来高效地训练它。当$\boldsymbol{w}^{\top} \boldsymbol{h}+b$属于单位区间外时，梯度为0。我们想要的是无论模型给出了什么错误答案，总能有一个较大的梯度。</p>
<p><strong>$sigmoid$输出单元：</strong></p>
<script type="math/tex; mode=display">
\hat{y}=\sigma( \boldsymbol{w}^{\top}\boldsymbol{h}+b)</script><p>其中$\sigma$是</p>
<script type="math/tex; mode=display">
\sigma(x)=\frac{1}{1+\exp (-x)}</script><p>因此这个输出单元由两部分组成：一个线性层计算$z=\boldsymbol{w}^{\top} \boldsymbol{h}+b$，一个激活函数层将$z$转化成概率。</p>
<p><strong>用 $z$的值定义 $y$的分布</strong>：<br>假定非归一化的对数概率对y和z是线性的</p>
<script type="math/tex; mode=display">
\log\tilde{P}(y) =y z</script><p>对它们取对数得到非归一化的概率</p>
<script type="math/tex; mode=display">
\tilde{P}(y) =\exp (y z)</script><p>对它归一化，除以一个合适的常数</p>
<script type="math/tex; mode=display">
P(y) =\frac{\exp (y z)}{\sum_{y^{\prime}=0}^{1} \exp \left(y^{\prime} z\right)}</script><p>发现这服从 Bernoulli 分布，该分布受 z 的 sigmoid 变换控制</p>
<script type="math/tex; mode=display">
P(y) =\sigma((2 y-1) z)</script><p><strong>损失函数</strong><br>这种在对数空间里预测概率的方法可以很自然地使用最大似然学习。因为用于最大似然的代价函数是 $− log P (y | x)$，代价函数中的 log 抵消了 sigmoid 中的 exp。如果没有这个效果， sigmoid 的饱和性会阻止基于梯度的学习做出好的改进。我们使用最大似然来学习一个由 sigmoid 参数化的 Bernoulli 分布，它的损失函数为：</p>
<script type="math/tex; mode=display">
\begin{aligned} J(\boldsymbol{\theta}) &=-\log P(y | \boldsymbol{x}) \\ &=-\log \sigma((2 y-1) z) \\ &=\zeta((1-2 y) z) \end{aligned}</script><p>$\zeta(x)=\log(1+\exp(x))$是<strong>softplus函数</strong>。</p>
<p>我们可以看到它仅仅在 $(1 − 2y)z$ 取绝对值非常大的负值时才会饱和。因此饱和只会出现在模型已经得到正确答案时——当 $y = 1$ 且 $z$ 取非常大的正值时，或者$y = 0$ 且 $z$ 取非常小的负值时。当 $z$ 的符号错误时， 变量 $(1 − 2y)z$可以简化为 $|z|$。当 $|z|$ 变得很大并且 $z$ 的符号错误时， softplus 函数渐近地趋向于它的变量 $|z|$。对 $z$ 求导则渐近地趋向于$sign(z)$。所以基于梯度的学习可以很快修正错误的$z$。</p>
<p><strong>其他损失</strong><br>当我们使用其他的损失函数，例如均方误差之类的，损失函数会在 $σ(z)$ 饱和时饱和。 sigmoid 激活函数在 $z$ 取非常小的负值时会饱和到 0，当 $z$ 取非常大的正值时会饱和到 1。这种情况一旦发生，梯度会变得非常小以至于不能用来学习，无论此时模型给出的是正确还是错误的答案。</p>
<h4 id="2-2-3-用于-Multinoulli-输出分布的-softmax-单元"><a href="#2-2-3-用于-Multinoulli-输出分布的-softmax-单元" class="headerlink" title="2.2.3 用于 Multinoulli 输出分布的 softmax 单元"></a>2.2.3 用于 Multinoulli 输出分布的 softmax 单元</h4><p>任何时候当我们想要表示一个具有 n 个可能取值的离散型随机变量的分布时，我们都可以使用 softmax 函数。它可以看作是 sigmoid 函数的扩展。softmax 函数最常用作分类器的输出，来表示 n 个不同类上的概率分布。</p>
<p>为了推广到具有 n 个值的离散型变量的情况，我们现在需要创造一个向量 $\hat{y}$，它的每个元素是 $\hat{y_i} = P(y = i | x)$。我们不仅要求每个 $\hat{y_i}$元素介于 0 和 1 之间，还要使得整个向量的和为 1，使得它表示一个有效的概率分布。用于 Bernoulli 分布的方法同样可以推广到 Multinoulli 分布。<br>首先，线性层预测了未归一化的对数概率：</p>
<script type="math/tex; mode=display">z=\boldsymbol{w}^{\top} \boldsymbol{h}+b</script><p>其中$z_{i}=\log \hat{P}(y=i | \boldsymbol{x})$。softmax 函数然后可以对 z 指数化和归一化来获得需要的 $\hat{y_i}$。最终， softmax 函数的形式为</p>
<script type="math/tex; mode=display">
\operatorname{softmax}(\boldsymbol{z})_{i}=\frac{\exp \left(z_{i}\right)}{\sum_{j} \exp \left(z_{j}\right)}</script><p><strong>损失函数</strong><br>最大化 $log P(y = i; z)=$</p>
<script type="math/tex; mode=display">
\log \operatorname{softmax}(z)_{i}=z_{i}-\log \sum_{j} \exp \left(z_{j}\right)</script><p>上式中当最大化对数似然时，第一项鼓励 $z_i$ 被推高，而第二项则鼓励所有的 $z$ 被压低。第二项$\log\sum_j \exp(z_j)$可以大致近似为 $\max_j z_j$。因为对任何明显小于 $\max_j z_j$ 的 $z_k$， $\exp(z_k)$ 都是不重要的。</p>
<p>因此我们可以感觉到，负对数似然代价函数总是强烈地惩罚最活跃的不正确预测。</p>
<p>而当正确答案已经具有了 softmax 的最大输入，那么 $−z_i$ 项和 $\log\sum_j \exp(z_j) ≈ \max_j z_j = z_i$项将大致抵消。这个样本对于整体训练代价贡献很小，这个代价主要由其他未被正确分类的样本产生。</p>
<p><strong>失败的损失函数</strong><br>除了对数似然之外的许多目标函数对 softmax 函数不起作用。具体来说，那些不使用对数来抵消 softmax 中的指数的目标函数，当指数函数的变量取非常小的负值时会造成梯度消失，从而无法学习。特别是，平方误差对于 softmax 单元来说是一个很差的损失函数，即使模型做出高度可信的不正确预测，也不能训练模型改变其输出。</p>
<p><strong>softmax的饱和</strong><br>当其中一个输入是最大（$z_i = \max_i z_i$）并且 $z_i$ 远大于其他的输入时，相应的输出 $\operatorname{softmax}(z)_i$ 会饱和到 1。当 $z_i$ 不是最大值并且最大值非常大时，相应的输出$\operatorname{softmax}(z)_i$ 也会饱和到 0。</p>
<h4 id="2-2-4-其他的输出类型"><a href="#2-2-4-其他的输出类型" class="headerlink" title="2.2.4 其他的输出类型"></a>2.2.4 其他的输出类型</h4><p>之前描述的线性、 sigmoid 和 softmax 输出单元是最常见的。神经网络可以推广到我们希望的几乎任何种类的输出层。最大似然原则给如何为几乎任何种类的输出层设计一个好的代价函数提供了指导。</p>
<p>一般的，如果我们定义了一个条件分布 $p(y | x; θ)$，最大似然原则建议我们使用$− \log p(y | x; θ)$ 作为代价函数。</p>
<p>一般来说，我们可以认为神经网络表示函数 $f(x; θ)$。这个函数的输出不是对 y值的直接预测。相反， $f(x; θ) = \omega$ 提供了 y 分布的参数。我们的损失函数就可以表示成 $− \log p(y; \omega(x))$。</p>
<p><strong>学习在给定 x 时， y 的条件高斯分布的方差</strong><br>例如，我们想要学习在给定 $x$ 时，$y$ 的条件高斯分布的方差。简单情况下，方差 σ2 是一个常数，此时有一个解析表达式，这是因为方差的最大似然估计量仅仅是观测值 y 与它们的期望值的差值的平方平均。一种计算上代价更加高但是不需要写特殊情况代码的方法是简单地将方差作为分布 $p(y | x)$ 的其中一个属性，这个分布由 ω = $f (x; θ)$ 控制。负对数似然 $− \log p(y; ω(x))$ 将为代价函数提供一个必要的合适项来使我们的优化过程可以逐渐地学到方差。在标准差不依赖于输入的简单情况下，我们可以在网络中创建一个直接复制到 ω 中的新参数。这个新参数可以是 $σ$ 本身，或者可以是表示 $σ^2$ 的参数 $v$，或者可以是表示 $1/(\sigma^2)$ 的参数 $β$，取决于我们怎样 $σ^2$对分布参数化。我们可能希望模型对不同的 $x$ 值预测出 $y$ 不同的方差。这被称为异方差(heteroscedastic)模型。</p>
<p><strong>多峰回归</strong><br>即预测条件分布 $p(y | x)$的实值，该条件分布对于相同的 $x$ 值在 $y$ 空间中有多个不同的峰值。<br>在这种情况下，使用高斯混合输出，条件分布定义为：</p>
<script type="math/tex; mode=display">
p(\boldsymbol{y} | \boldsymbol{x})=\sum_{i=1}^{n} p(\mathrm{c}=i | \boldsymbol{x}) \mathcal{N}\left(\boldsymbol{y} ; \boldsymbol{\mu}^{(i)}(\boldsymbol{x}), \boldsymbol{\Sigma}^{(i)}(\boldsymbol{x})\right)</script><p>使用高斯混合输出的神经网络被称为<strong>混合密度网络</strong>。<br>从上式中看到，神经网络必须有三个输出：</p>
<p>(1)定义$p(c = i | x)$ 的向量：它们由潜变量 $c$ 关联着，在 $n$ 个不同组件上形成 Multinoulli 分布。</p>
<p>(2)对所有的 $i$ 给出 $µ^{(i)}(x)$ 的矩阵：它们指明了与第 $i$ 个高斯组件相关联的中心或者均值。</p>
<p>(3)以及对所有的 $i$ 给出 $Σ^{(i)}(x)$ 的张量：它们指明了每个组件 $i$ 的协方差矩阵。</p>
<p><img src="https://img-blog.csdnimg.cn/20191102171006216.png" width="80%" alt></p>
<h2 id="3-隐藏单元"><a href="#3-隐藏单元" class="headerlink" title="3 隐藏单元"></a>3 隐藏单元</h2><p>隐藏单元的设计是一个非常活跃的研究领域，并且还没有许多明确的指导性理论原则。</p>
<p>ReLu是隐藏单元极好的默认选择。许多其他类型的隐藏单元也是可用的。决定何时使用哪种类型的隐藏单元是困难的事，通常不可能预先预测出哪种隐藏单元工作得最好。设计过程充满了试验和错误，先直觉认为某种隐藏单元可能表现良好，然后用它组成神经网络进行训练，最后用验证集来评估它的性能。</p>
<p><strong>不可微性</strong><br>例如， ReLU $g(z) = max\{0, z\}$ 在 z = 0 处不可微。这似乎使得 g 对于基于梯度的学习算法无效。在实践中，梯度下降对这些机器学习模型仍然表现得足够好。部分原因是神经网络训练算法通常不会达到代价函数的局部最小值，而是仅仅显著地减小它的值。因为我们不再期望训练能够实际到达梯度为 0 的点，所以代价函数的最小值对应于梯度未定义的点是可以接受的。不可微的隐藏单元通常只在少数点上不可微。一般来说，函数 g(z) 具有左导数和右导数，左导数定义为紧邻在 z 左边的函数的斜率，右导数定义为紧邻在 z 右边的函数的斜率。神经网络训练的软件实现通常返回左导数或右导数的其中一个，而不是报告导数未定义或产生一个错误。当一个函数被要求计算 g(0) 时，底层值真正为 0 是不太可能的。相对的，它可能是被舍入为 0 的一个小量 ϵ。在实践中，我们可以忽略隐藏单元激活函数的不可微性。</p>
<p><strong>形式</strong><br>大多数的隐藏单元都可以描述为接受输入向量 $x$，计算仿射变换 $z = W^⊤x + b$，然后使用一个逐元素的非线性函数 $g(z)$。大多数隐藏单元的区别仅仅在于激活函数 $g(z)$ 的形式。</p>
<h3 id="3-1-整流线性单元及其扩展"><a href="#3-1-整流线性单元及其扩展" class="headerlink" title="3.1 整流线性单元及其扩展"></a>3.1 整流线性单元及其扩展</h3><p>ReLU使用激活函数 $g(z) = max\{0, z\}$</p>
<p>整流线性单元易于优化，因为它们和线性单元非常类似。唯一区别在于整流线性单元在其一半的定义域上输出为零。这使得只要整流线性单元处于激活状态，它的导数都能保持较大。它的梯度不仅大而且一致。整流操作的二阶导数几乎处处为 0，并且在整流线性单元处于激活状态时，它的一阶导数处处为 1。这意味着相比于引入二阶效应的激活函数来说，它的梯度方向对于学习来说更加有用。</p>
<p><strong>ReLU扩展</strong><br>整流线性单元的三个扩展基于当 $z_i &lt; 0$ 时使用一个非零的斜率 $α_i$：$h_i = g(z; α)_i = max(0; z_i) + α_i min(0; z_i)$</p>
<ul>
<li><strong>absolute value rectification</strong><script type="math/tex; mode=display">g(z) = |z|</script></li>
<li><strong>Leaky ReLU</strong><br>将 $α_i$ 固定成一个类似 0.01 的小值。</li>
<li><strong>parametric ReLU</strong> 或者 <strong>PReLU</strong><br>将 $α_i$ 作为一个学习的参数。</li>
</ul>
<p><strong>maxout</strong><br>进一步扩展了整流线性单元。 maxout 单元将 z 划分为每组具有 k 个值的组，而不是使用作用于每个元素的函数 g(z)。每个maxout 单元则输出每组中的最大元素：</p>
<script type="math/tex; mode=display">
g(\boldsymbol{z})_{i}=\max _{j \in \mathbb{G}^{(i)}} z_{j}</script><p>这提供了一种方法来学习对输入 x 空间中多个方向响应的分段线性函数。</p>
<p>maxout 单元可以学习具有多达 k 段的分段线性的凸函数。 maxout 单元因此可以视为学习激活函数本身而不仅仅是单元之间的关系。使用足够大的 k， maxout 单元可以以任意的精确度来近似任何凸函数。特别地，k=2 的 maxout 层可以学习实现和传统层相同的输入 x 的函数，这些传统层可以使用ReLU、 absolute value rectification、 Leaky ReLU 或 parametric ReLU，或者可以学习实现与这些都不同的函数。 </p>
<p>每个 maxout 单元现在由 k 个权重向量来参数化，而不仅仅是一个，所以 maxout单元通常比整流线性单元需要更多的正则化。</p>
<p>maxout 单元在某些情况下，要求更少的参数，可以获得一些统计和计算上的优势。如果由 n 个不同的线性过滤器描述的特征可以在不损失信息的情况下，用每一组 k 个特征的最大值来概括的话，那么下一层的权重数可以减少为1/k。</p>
<p>因为每个单元由多个过滤器驱动， maxout 单元具有一些冗余来帮助它们抵抗一种被称为 <strong>灾难遗忘</strong>（catastrophic forgetting）的现象，这个现象是说神经网络忘记了如何执行它们过去训练的任务 。</p>
<p>整流线性单元和它们的这些扩展都是基于一个原则，那就是如果它们的行为更接近线性，那么模型更容易优化。</p>
<h3 id="3-2-logistic-sigmoid与双曲正切函数"><a href="#3-2-logistic-sigmoid与双曲正切函数" class="headerlink" title="3.2 logistic sigmoid与双曲正切函数"></a>3.2 logistic sigmoid与双曲正切函数</h3><p>logistic sigmoid 激活函数</p>
<script type="math/tex; mode=display">
g(z)=\sigma(z)=\frac{1}{1+\exp (-z)}</script><p>双曲正切激活函数</p>
<script type="math/tex; mode=display">
g(z)=tanh(z)</script><p>关系</p>
<script type="math/tex; mode=display">
tanh(z)=2\sigma(2z)-1</script><p><img src="https://img-blog.csdnimg.cn/2019110320464335.png" width="80%" alt></p>
<p>与分段线性单元不同， sigmoid 单元在其大部分定义域内都饱和——当 z 取绝对值很大的正值时，它们饱和到一个高值，当 z 取绝对值很大的负值时，它们饱和到一个低值，并且仅仅当 z 接近 0 时它们才对输入强烈敏感。 sigmoid 单元的广泛饱和性会使得基于梯度的学习变得非常困难。因为这个原因，现在不鼓励将它们用作前馈网络中的隐藏单元。当使用一个合适的代价函数来抵消 sigmoid 的饱和性时，它们作为输出单元可以与基于梯度的学习相兼容。</p>
<p>当必须要使用 sigmoid 激活函数时，双曲正切激活函数通常要比 logistic sigmoid 函数表现更好。<br>在 $tanh(0) = 0$ 而 $σ(0) = 0.5$ 的意义上，它更像是单位函数。因为 $tanh$ 在 0 附近与单位函数类似，训练深层神经网络 $\hat{y} =  \boldsymbol{w}^⊤tanh( \boldsymbol{U}^⊤tanh( \boldsymbol{V}^⊤x))$类似于训练一个线性模型 $\hat{y} =  \boldsymbol{w}^{⊤}  \boldsymbol{U}^{⊤} \boldsymbol{V}^{⊤}x$，只要网络的激活能够被保持地很小。这使得训练 tanh 网络更加容易。</p>
<p>sigmoid 激活函数在除了前馈网络以外的情景中更为常见。循环网络、许多概率模型以及一些自编码器有一些额外的要求使得它们不能使用分段线性激活函数，并且使得 sigmoid 单元更具有吸引力，尽管它存在饱和性的问题。</p>
<p><img src="https://img-blog.csdnimg.cn/20191103205157560.png" width="100%" alt></p>
<h3 id="3-3-其他隐藏单元"><a href="#3-3-其他隐藏单元" class="headerlink" title="3.3 其他隐藏单元"></a>3.3 其他隐藏单元</h3><p>也存在许多其他种类的隐藏单元，但它们并不常用。</p>
<p>一般来说，很多种类的可微函数都表现得很好。许多未发布的激活函数与流行的激活函数表现得一样好。为了提供一个具体的例子，作者在 MNIST 数据集上使用 h = cos(Wx + b) 测试了一个前馈网络，并获得了小于 1% 的误差率，这可以与更为传统的激活函数获得的结果相媲美。在新技术的研究和开发期间，通常会测试许多不同的激活函数，并且会发现许多标准方法的变体表现非常好。这意味着，通常新的隐藏单元类型只有在被明确证明能够提供显著改进时才会被发布。新的隐藏单元类型如果与已有的隐藏单元表现大致相当的话，那么它们是非常常见的，不会引起别人的兴趣。</p>
<p><strong>不使用激活函数（或$g(z)=z$）</strong><br>我们已经看过线性单元可以用作神经网络的输出。它也可以用作隐藏单元。如果神经网络的每一层都仅由线性变换组成，那么网络作为一个整体也将是线性的。然而，神经网络的一些层是纯线性也是可以接受的。考虑具有 $n$ 个输入和 $p$个输出的神经网络层 $h = g(W^⊤x + b)$。我们可以用两层来代替它，一层使用权重矩阵 $U$，另一层使用权重矩阵 $V$。如果第一层没有激活函数，那么我们对基于 $W$ 的原始层的权重矩阵进行因式分解。分解方法是计算 $h = g(V^⊤U^⊤x + b)$。如果 $U$ 产生了 $q$ 个输出，那么$U$ 和 $V$ 一起仅包含 $(n + p)q$ 个参数，而 $W$ 包含 $np$ 个参数。</p>
<p>如果 $q$ 很小，这可以在很大程度上节省参数。这是以将线性变换约束为低秩的代价来实现的，但这些低秩关系往往是足够的。线性隐藏单元因此提供了一种减少网络中参数数量的有效方法。</p>
<p><strong>softmax单元</strong><br>softmax是另外一种经常用作输出的单元（如第 6.2.2.3 节中所描述的），但有时也可以用作隐藏单元。 softmax 单元很自然地表示具有 k 个可能值的离散型随机变量的概率分布，所以它们可以用作一种开关。这些类型的隐藏单元通常仅用于明确地学习操作内存的高级结构中。</p>
<p>其他一些常见的隐藏单元类型包括：</p>
<ul>
<li><strong>径向基函数</strong>（radial basis function, RBF）<script type="math/tex; mode=display">h_{i}=\exp \left(-\frac{1}{\sigma_{i}^{2}}\left\|\boldsymbol{W}_{:, i}-\boldsymbol{x}\right\|^{2}\right)</script>这个函数在 x 接近模板 $W_{:,i}$ 时更加活跃。因为它对大部分 x 都饱和到 0，因此很难优化。</li>
<li><strong>softplus函数</strong>： $g(a) = ζ(a) = log(1 + e^a)$。这是ReLU单元的平滑版本，由 Dugas et al. (2001) 引入用于函数近似，由 Nair and Hinton (2010a) 引入用于无向概率模型的条件分布。 Glorot et al. (2011a) 比较了 softplus 和ReLU单元，发现后者的结果更好。通常不鼓励使用 softplus 函数。 softplus 表明隐藏单元类型的性能可能和直觉相反——因为它处处可导、不完全饱和，人们可能希望它具有优于整流线性单元的点，但根据经验来看，它并没有。</li>
<li><strong>硬双曲正切函数（hard tanh）</strong><script type="math/tex; mode=display">g(a) = max(−1, min(1; a))</script>它的形状和 tanh 以及ReLU类似，但是不同于后者，它是有界的 。<br><img src="https://img-blog.csdnimg.cn/20191104113314736.png" width="80%" alt></li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Machine-Learning/" rel="tag"><i class="fa fa-tag"></i> Machine Learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/10/12/DeepLearning-Chapter-5/" rel="next" title="DeepLearning Chapter 5">
                <i class="fa fa-chevron-left"></i> DeepLearning Chapter 5
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Loy Fan</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#深度前馈网络"><span class="nav-number">1.</span> <span class="nav-text">深度前馈网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-实例：学习XOR"><span class="nav-number">1.1.</span> <span class="nav-text">1 实例：学习XOR</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-基于梯度的学习"><span class="nav-number">1.2.</span> <span class="nav-text">2 基于梯度的学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-输出单元"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.2 输出单元</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-用于高斯输出分布的线性单元"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">2.2.1 用于高斯输出分布的线性单元</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-用于-Bernoulli-输出分布的-sigmoid-单元"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">2.2.2 用于 Bernoulli 输出分布的 sigmoid 单元</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-3-用于-Multinoulli-输出分布的-softmax-单元"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">2.2.3 用于 Multinoulli 输出分布的 softmax 单元</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-4-其他的输出类型"><span class="nav-number">1.2.1.4.</span> <span class="nav-text">2.2.4 其他的输出类型</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-隐藏单元"><span class="nav-number">1.3.</span> <span class="nav-text">3 隐藏单元</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-整流线性单元及其扩展"><span class="nav-number">1.3.1.</span> <span class="nav-text">3.1 整流线性单元及其扩展</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-logistic-sigmoid与双曲正切函数"><span class="nav-number">1.3.2.</span> <span class="nav-text">3.2 logistic sigmoid与双曲正切函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-其他隐藏单元"><span class="nav-number">1.3.3.</span> <span class="nav-text">3.3 其他隐藏单元</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Loy Fan</span>

  
</div>








<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">49.2k words in this blog site.</span>
</div>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
